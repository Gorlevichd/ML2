\documentclass[a4paper, 12pt]{article}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian, english]{babel}
\usepackage{statmath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\setlength\parindent{0pt}
\catcode`@=11
\def\caseswithdelim#1#2{\left#1\,\vcenter{\normalbaselines\m@th
  \ialign{\strut$##\hfil$&\quad##\hfil\crcr#2\crcr}}\right.}% you might like it without the \strut
\catcode`@=12
%
\def\bcases#1{\caseswithdelim[{#1}}
\def\vcases#1{\caseswithdelim|{#1}}
%

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\title{МО2}
\date{March 2021}

\begin{document}

\maketitle

\part{Лекции}

\section{Ядровые методы}

Данные: $x = (x_{1}, ... x_{m})$
\newline
Базисные функции: $\phi(x_{1}, ...)$
\newline
Модель принимает вид: $a(x) = \sum_{j = 1}^{m}w_{j}\phi_{j}(x)$
\newline
Для хорошего качества нужно много базисных функций $\rightarrow$ Ядровые методы позволяют не перебирать большое количество базисных функций

\begin{itemize}
    \item Быстрое обучение
\end{itemize}

\begin{center}
    \textit{Ядровые методы}
\end{center}
\begin{enumerate}
    \item Двойственное представление для линейной регрессии
    \newline
    
    $Q(w) = \frac{1}{2}\sum_{i = 1}^{l}(\sum_{j = 1}^{m}(w_{j}\phi_{j}(x_{i}) - y_{i})^{2} + \frac{\lambda}{2}||w||^{2}_{2} = \frac{1}{2}||\Phi w - y||_{2}^{2} + \frac{\lambda}{2}||w||_{2}^{2}$
    \newline
    
    $\Phi = \begin{pmatrix}\phi_{1}(x_{1}) & ... & \phi_{m}(x_{1}) \\ ... & ... & ... \\ \phi_{1}(x_{l}) & ... & \phi_{m}(x_{l}) \end{pmatrix}$
    
    $\nabla_{w}Q = \Phi^{T}(\Phi w - y) + \lambda w \rightarrow w = -\frac{1}{\lambda}\Phi^{T}(\Phi w - y) \rightarrow w = \Phi^{T}a$
    \newline
    
    w является линейной комбинацией строк $\Phi \rightarrow$ Решение можно искать из $w = \Phi^{T}a$ 
    
    $Q(a) = \frac{1}{2}||\Phi \Phi^{T}a - y|| + \frac{\lambda}{2}a^{T} \Phi \Phi^{T}a \rightarrow min_{a}$
    \newline
    
    $\Phi \Phi^{T}$ - матрица Грама (попарных скалярных произведений объектов)
    \newline
    
    Можно записать Q(w) так, что он зависит только от скалярных произведений объектов
    
    \item SVM
    
    $\begin{cases}
    \sum_{i = 1}^{l} \lambda_{i} - \frac{1}{2} \sum_{i, j = 1}^{l} \lambda_{i}\lambda_{j}y_{i}y_{j}<x_{i}, x_{j}> \rightarrow \max_{\lambda} \\
    0 \geq \lambda_{i} \leq C
    \\
    \sum_{i = 1}^{l} \lambda_{i}y_{i} = 0
    \end{cases}$
    
    Такая формулировка задачи зависит от скалярных произведений объектов
    
    \item Алгоритм
    \begin{enumerate}
        \item Добавляем новые признаки
        \item $x, z \in X$
        \item Делаем это так, что $<\phi(x), \phi(z)>$ выражается через $<x, z>$
        \item Используем метод, который использует скалярные произведения объектов
        \item В этом методе $<x, z> \rightarrow <\phi(x), \phi(z)>$ (\textit{Kernel trick})
    \end{enumerate}
    \item \textbf{Ядро} - функция $K(x, z) = <\phi(x), \phi(z)>$, где $\phi: X \to H$
    \begin{enumerate}
        \item H - спрямляющее пространство
        \item $\phi$ - спрямляющее отображение
    \end{enumerate}
    
    \item \textbf{Теорема Мерсера}
    \begin{enumerate}
        \item $K(x, z) \text{ - ядро} \leftrightarrow \begin{cases}
        K(x, z) = K(z, x) \\ K \text{ неотрицательно определенная}
        \end{cases}$
        \item НО $ \rightarrow \forall l, \forall (x_{1}, ..., x_{l}) \in R^{d} \rightarrow (K(x_{i}, x_{j}))^{l}_{i, j = 1} \text{ НО}$
        \item \textit{На практике теорема Мерсера слишком сложна для применения}
    \end{enumerate}
    
    \item \textbf{Теорема 1}
    \begin{enumerate}
        \item Если
        \begin{enumerate}
            \item $K_{1}(x, z), K_{2}(x, z)$ - ядра, $x, z \in X$
            \item $f^{(x)}$ - вещественная функция на X
            \item $\phi: X \rightarrow R^{n}$
            \item $K_{3}$ - ядро заданное на $R^{n}$
        \end{enumerate}
        \item То \textit{cледующие функции являюися ядрами:}
        \begin{enumerate}
            \item $K(x, z) = K_{1}(x, z) + K_{2}(x, z)$
            \item $K(x, z) = \alpha K_{1}(x, z)$
            \item $K(x, z) = K_{1}K_{2}$
            \item $K(x, z) = f^{(x)}f^{(z)}$
            \item $K(x, z) = K(\phi(x), \phi(z))$
        \end{enumerate}
    \end{enumerate}
    \item \textbf{Теорема 2}
    \begin{enumerate}
        \item Если:
        \begin{enumerate}
                \item $K_{1}(x, z), K_{2}(x, z), ...$ - последовательность ядер
                \item $\exists K(x, z) = \lim_{n \to \infty}K_{n}(x, z), \forall x, z$
        \end{enumerate}
        \item То:
        \begin{enumerate}
            \item K - ядро
        \end{enumerate}
    \end{enumerate}
    \item \textbf{Полиномиальные ядра}
    \begin{enumerate}
        \item p(v) - многочлен с неотриц. коэфф
        \item $K(x, z) = w_{0} + w_{1}<x, z> + w_{2}<x, z>^{2} + ...$
        \item Является ядром по теореме 1
        \item $K(x, z) = (<x, z> + R)^{m} = \sum_{i = 0}^{m} C_{m}^{i}R^{m - i}<x, z>^{i}$
        \begin{enumerate}
            \item Если расписать все $<x, z>^{i}$, то получим все мономы степени i от исходных признаков 
            \item Зачем R? $\rightarrow$ коэффициент при мономе = $\sqrt{C_{m}^{i}R^{m - i}}$
            \item Сравним веса при мономах 1 и (m - 1)
            $\sqrt{\frac{C^{m - 1}_{m}R}{C^{1}_{m}R^{m-1}}} = \sqrt{\frac{1}{R^{m - 2}}}$
            \item R больше - мономы высоких степеней имеют низкий вклад
            \item Конечномерное спрямляющее пространство, но можно сделать линейно разделимое пространство
        \end{enumerate}
    \end{enumerate}
    \item \textbf{Гауссовы ядра}
    \begin{enumerate}
        \item Позволяет перевести в бесконечномерное спрямляющее пространство
        \item \fbox{$K(x, z) = exp\left(-\frac{||x - z||^{2}}{2\sigma^{2}}\right)$}
        \begin{enumerate}
            \item $exp(<x, z>) = \sum_{k = 0}^{\infty}\frac{<x, z>^{k}}{k\!}, \forall x, z = \lim_{n \to \infty}  \sum_{k = 0}^{\infty}\frac{<x, z>^{k}}{k\!}$
            \begin{enumerate}
                \item Разложение через ряд Тейлора
                \item Ядро, как последовательность ядер
            \end{enumerate}
            \item $\frac{exp(<x, z>)}{2\sigma^{2}}$ - ядро, аналогично
            \item $exp\left(-\frac{||x - z||^{2}}{2\sigma^{2}}\right) = exp\left(-\frac{<x - z, x - z>}{2\sigma^{2}}\right) = exp\left(-\frac{<x,x> -  <z, z>,  + <x, z>}{2\sigma^{2}}\right) = \frac{exp(<x, z> / \sigma^{2}}{exp(||x||^{2} / \sigma^{2})exp(||z||^{2} / \sigma^{2}})$
            \item $exp(<x, z> / \sigma^{2}) = K(x, z) = <\phi(x), \phi(z)>$
            \item $\tilde{\phi(x)} = \frac{\phi(x)}{||\phi(x)||} = \frac{\phi(x)}{\sqrt{K(x, x)}}$
            \item $<\tilde{\phi(x)}, \tilde{\phi(z)}> = \frac{<\phi(x), \phi(z)>}{\sqrt{K(x, x)K(z, z)}}$
        \end{enumerate}
        \item Какое спрямляющее пространство? - бесконечная сумма всех мономов
        \item \textit{Утверждение:} $x_{1}, ..., x_{l}$ - различные векторы из $\mathbb{R}^{d}$
        
        Тогда:
        
        $G = (exp\left(-\frac{||x - z||^{2}}{2\sigma^{2}}\right))^{l}_{i, j = 1}$ - невырожденная при $\sigma^{2} > 0$
        \item $x_{1}, ..., x_{l} \in \mathbb{R}^{d}$ - их матрица Грамма невырождена $\rightarrow \phi(x_{1}, ..., x_{l})$ ЛНЗ $\rightarrow$ бесконечное количество ЛНЗ векторов $\rightarrow$ бесконечномерное пространство
    \end{enumerate}
    \item \textbf{Ядровой SVM}
    \begin{enumerate}
        \item $ \begin{cases}
        \frac{1}{2}||w||^{2} + C\sum_{i = 1}^{l} \xi_{i} \rightarrow min_{w, b, \xi} \\
        y_{i}(<w, x_{i}> + b) \geq 1 - \xi_{i} \\
        \xi_{i} \geq 0
        \end{cases} $
        \[L(w, b, \xi, \lambda, \mu) = \frac{1}{2}||w||^2 + C\sum_{i = 1}^{d}\xi_i - \sum_{i = 1}^{l}\lambda_i (y_i(<w, x_i> + b) - 1 + \xi_i) - \sum_{i = 1}^l \mu_i \xi_i\]
        
        В точке оптимума $\nabla_w L = 0$
        
        \[\nabla_w L = w - \sum_{i = 1}^l \lambda y_i x_i = 0 \rightarrow w = \sum_{i = 1}^l \lambda_i y_i x_i\]
        \[\nabla_b L = \sum_{i = 1}^{l} \lambda_i y_i = 0\]
        \[\nabla_{\xi_i} L = C - \lambda_i - \mu_i = 0 \rightarrow \lambda_i + \mu_i = C\]
        
        Условие дополняющей нежесткости:
        
        \[\lambda_i (y_i(<w, x_i> + b) - 1 + \xi_i) = 0 \rightarrow \lambda_i = 0 \textrm{ или } (y_i(<w, x_i> + b) - 1 + \xi_i) =0\]
        \[\mu_i \xi_i = 0 \rightarrow \mu_i = 0 \textrm{ или } \xi_i = 0\]

        Свойства лагранжиана:

        \[\lambda \geq 0, \mu \geq 0\]

        \item Типы объектов
        \begin{enumerate}
            \item $\lambda_i = 0 \rightarrow \mu_i = C \rightarrow \xi_i = 0 \rightarrow x_i$ лежит с правильной стороны от 
            разделяющей гиперплоскости и на достаточном расстоянии от нее. $w = \sum_{i = 1}^l \lambda y_i x_i \rightarrow$ объект не влияет на веса. 
            Называется \textbf{периферийный.}
            \item $0 < \lambda_i < 1 \rightarrow \mu \neq 0 \rightarrow \xi_0 = 0$. $x_i$ не залезает на разделяющую полосу, но $y_i(<w, x_i> + b) = 1 \rightarrow x_i$ лежит прямо на границе.
            Дает вклад в $w$. $x_i$ - \textbf{опорный граничный.}
            \item $\lambda_i = C \rightarrow \xi_i > 0$. 
            $x_i$ дает вклад в w. $\xi_i > 0 \rightarrow x_i$ нарушает границу - \textbf{Опорные нарушители}.      
        \end{enumerate}
        \item Подставляем $w = \sum_{i = 1}^l \lambda y_i x_i$ в лагранжиан, учтем ограничения $\sum_{i = 1}^l \lambda_i y_i = 0$ и $C - \lambda_i - \mu_i = 0$
        
        \textbf{Двойственная задача SVM}
        \[\begin{cases}
            L = \sum_{i = 1}^l \lambda_i - \frac{1}{2}\sum_{i, j = 1}^l \lambda_i \lambda_j y_i y_j <x_i, x_j> \rightarrow \max_{\lambda} \\
            \sum_{i = 1}^l \lambda_i y_i = 0 \\
            0 \leq \lambda_i \leq C
        \end{cases}\]

        \item Если $\lambda$ - решение, то $w = \sum_{i = 1}^l \lambda_i y_i x_i$ - решение исходной задачи
        \item Задача зависит от объектов только через скалярное произведение $\rightarrow$
        можно заменить его на ядро
        \item Находим b
        Берем $x_i: 0 < \lambda_i < C \rightarrow \xi_i = 0 \rightarrow y_i(<w, x_i> + b) = 1 \rightarrow b = y_i - <w, x_i>$
        \item Минусы ядрового SVM
        \begin{enumerate}
            \item Сложно контролировать переобучение
            \item Необходимо хранить в памяти матрицу Грамма
            \item Нельзя менять функцию потерь
        \end{enumerate}
    \end{enumerate}
    \item \textbf{Применение ядерной модели}
    \begin{enumerate}
        \item $a(x) = sign(<w, x> + b) = sign(<\sum_{i = 1}^l \lambda y_i x_i, x> + b) = sign(\sum_{i = 1}^l \lambda_i y_i <x_i, x> + b)$ 
    \end{enumerate}
\end{enumerate}

\section{Аппроксимации ядер, EM алгоритм}

Скалярные произведения тяжело хранить из-за
размера матрицы. 

Есть ли возможность построить 
$\tilde{\phi}(x) \rightarrow 
<\tilde{\phi}(x_i), \tilde{\phi}(x_j)> 
\approx K(x_i, x_j)$

\subsection{Метод случайных признаков Фурье}

\[K(x, z) = K(x - z)\]

K - непрерывная функция

\textit{Теорема Бохнера}

\[K(x - z) \rightarrow \exists p(w) \rightarrow 
K(x - z) = \int_{R^{d}} p(w)e^{iw^{T}(x - z)}dw\]

\textit{Используем:}

\[K(x - z) = \int_{R^{d}} p(w)e^{iw^{T}(x - z)}dw 
\xrightarrow{\textrm{Формула Эйлера \footnotemark[1]}} 
\int_{R^{d}} p(w)cos(w^{T}(x - z)) + i\int_{R^{d}} p(w)cos(w^{T}(x - z))\] 
\[\xrightarrow{K(x - z) \textrm{ - веществ.}} 
\textrm{ Комплексная часть = 0} \rightarrow 
K(x - z) = \int_{R^{d}} p(w)cos(w^{T}(x - z))dw\]
\[\xrightarrow{ \textrm{Монте-Карло \footnotemark[2]}}
K(x - z) \approx\{w_j \sim p(w)\}: 
\frac{1}{n} \sum_{i = 1}^{n} cos w_j^T(x - z)\]
\[= \frac{1}{n} \sum_{i = 1}^n cos w_j^Tx cos w_j^Tz + sin w_j^Tx sin w_j^Tz\]
\footnotetext[1]{$e^{ix} = cosx + isinx$}
\footnotetext[2]{$\int_{a}^{b}f(x)dx = \frac{b - a}{n} \sum_{i = 1}^N f(u_i)$}

\[\tilde{\phi}(x) = \frac{1}{n}(cos w_1^Tx, \ldots, cos w_n^Tx, sin w_1^Tx, \ldots, sin w_n^Tx)\]
\[K(x - z) = <\tilde{\phi}(x), \tilde{\phi}(z)>\]

Для гауссова ядра:

\[p(w) = \mathcal{N}(0, 1)\]

\subsection{EM алгоритм}

Смесь распределений: 

\[\begin{cases}
    p(x) = \sum_{k = 0}^{K} \pi_k p_k(x) \\
    \sum \pi_k = 1    
\end{cases}
\]

Вероятностный эксперимент:

Выбираем K из $[\pi_1, \ldots, \pi_K]$, выбираем x из $pi_k(x)$

Z - скрытые переменные

\[Z = \{0, 1\}^K, \sum Z_k = 1\]
\[p(Z_k = 1) = \pi_k\]
\[p(z) = \prod_{k = 1}^K \pi_k^{Z_k}\]
\[p(x \mid Z_k = 1) = p_k(x)\]
\[p(x \mid z) = \prod_{k = 1}^{K} (p_{k}(x)^{Z_{k}})\]
\[p(x, z) = p(x \mid z)p(z) = \prod_{k = 1}^{K} (\pi_k p_k(x))^{Z_{k}}\]
\[p(x) = \sum_{k = 1}^{K} p(x, z = k) = \sum_{k = 1}^{K} \pi_k p_k(x)\]

Вероятностная кластеризация:

\(p_k(x)\) - распределение k-го кластера

\[x \rightarrow (p_1(x), \ldots, p_k(x))\]

Хотим описать X смесью распределений

\[p(x) = \sum_{k = 1}^K \pi_k \phi(x \mid \theta_k), \phi(x \mid \theta_k) \sim \mathcal{N}(\mu, \Sigma), \theta = (\mu, \Sigma)\]

Неполное правдоподобие:

\[ln(P(X \mid \Theta)) = \sum_{i = 1}^{l} log \sum_{k = 1}^K \pi_k \phi(x_i \mid \theta_k) \rightarrow max_{\theta}\]

Логарифм многооптимальная функция - просто оптимизировать ее сложно

Используем функцию полного правдоподобия

\[log(P, X \mid \Theta) = \sum_{i = 1}^{l} log \sum_{k = 1}^{K} (\pi_k \phi(x_i \mid \theta_k))^{Z_k}\]
\[\sum_{i = 1}^{l}\sum_{k = 1}^{K} Z_{ik}(log\pi_k + log\phi(x_i \mid \theta_k)) \rightarrow \max_{\Theta}\]

Известно аналитическое решение для нормального распределения.

Не знаем $Z_ik$

\[\Theta = (\pi_1, \ldots, \pi_k, \theta_1, \ldots, \theta_k)\]

Используем метод ALS для поиска $Z, \Theta$

\begin{enumerate}
    \item Оптимизация по скрытым переменным
    
    Апостериорное распределение: \(p(Z \mid X, \Theta) = \frac{P(X, Z \mid \Theta)}{p(X \mid \Theta)}\)

    \[Z^{\star} = \argmax_{Z} p(Z \mid X, \Theta)\]

    \item Оптимизировать по $\Theta$
    
    \[log p(X, Z^{\star} \mid \Theta) \rightarrow \max_{\Theta}\]
    \item Повторять до сходимости
\end{enumerate}

Можно лучше. Не гарантирует сходимости


EM-алгоритм - метод обучения моделей со скрытыми переменными

\textbf{EM-алгоритм}

\begin{enumerate}
    \item E-шаг - вычисляем $p(Z \mid X, \Theta)$ и запоминаем
    \item M-шаг
    
    \[E_{Z \sim p(Z \mid X, \Theta)}log p(X, Z \mid \Theta) = 
    \sum_{Z} p(Z \mid X, \Theta)log p(X, Z \mid \Theta) 
    \rightarrow \max_{\Theta}\]
\end{enumerate}

Вывод EM-алгоритма

\[log p(X \mid \Theta) = Z(q, \Theta) + KL(q \mid \mid p)\]

\[L(q, \Theta) = \sum_{Z} q(Z)log \frac{p(X, Z \mid \Theta)}{q(Z)}\]

\[KL(q \mid \mid p) = -\sum_{Z} q(Z)log \frac{p(Z \mid X, \Theta)}{q(Z)}\]

\[\forall q(Z)\]

\(L(q, \Theta)\) - нижняя оценка

Берем \(q(Z) = p(Z \mid X, \Theta)\) - получаем E-шаг

\(L(q, \Theta) = \sum_{Z \sim q(Z)} p(Z) log(...)\) - М-шаг

EM-алгоритм дает гарантии на рост правдоподобия

\section{ЕМ алгоритм 2}

\textbf{Свойства}

\begin{enumerate}
    \item \(logP(X \mid \Theta^{new}) \geq logP(X \mid \Theta^{old})\)
    \item Если $\Theta_{i}$ не является станционарной точкой l, 
    то $\Theta_{i + 1} \neq \Theta_{i}$

    \[\nabla l(\Theta_i) \neq 0\]
    \[logP(X \mid \Theta_i) = L(q \mid \Theta_i) + 
    KL(q(\Theta_i) \mid\mid p)\]
    \[KL = 0 \rightarrow 
    \nabla_{\Theta}KL(q( \mid \mid p) = 0 \rightarrow 
    \nabla L(q \mid \Theta_i) \neq 0 \rightarrow\]
    \[\textrm{На М шаге точно сдвинемся и поменяем }\Theta\]
\end{enumerate}

\textbf{Теорема}

\[Q(\Theta, \Theta^{Old}) = 
E_{z \sim p(Z \mid X, \Theta^{Old})}
logP(Z, X \mid \Theta^{Old})\]

Пусть Q непрерывна по обоим аргументам

Тогда:

\begin{enumerate}
    \item Все предельные точки последовательности $\Theta$
    являются станционарными точками $logP(X \mid \Theta)$
    \item $logP(X \mid \Theta)$ монотонно сходится к 
    $logP(X \mid \Theta^{\star})$ - одной из станционарных точек 
\end{enumerate}

Отвлеченная штука:

X - обучающая выборка

Хотим подогнать под нее распределение $p(x \mid \theta)$

Эмпирическое распределение:

\[\hat{p}(x \mid X) = \frac{1}{l}\sum^{l}_{i = 1}[x = x_i]\]

Минимизировать KL-дивергенцию между эмпирическим и 
параметрическим распределением.

\[KL(\hat{p}(x \mid X) \mid \mid p(x \mid \theta)) 
\rightarrow min_{\theta}\]

\[= \sum_{i = 1}^{l} 
\frac{1}{l}log  \frac{1 / l}{p(x_i \mid \theta)} = 
\sum_{i = 1}^{l} 
\frac{1}{l}log(1 / l) - logp(x_i \mid \theta) \rightarrow \]
\[\sum_{i = 1}{l} logP(x_i \mid \theta) \rightarrow \max_{\theta}\]

\section{Поиск аномалий}

В обучении есть только один класс - неаномальный, 
надо научится отделять от него аномалии

\textbf{Несбалансированная классификация}
\begin{enumerate}
    \item (Under/over)sampling - взвешенный 
    функционал ошибки
    \item Синтетические объекты
    \begin{enumerate}
        \item SMOTE
        \begin{enumerate}
            \item Выбираем объекты $X_1$ из минорного класса,
            выбираем случаный объект из k ближайших соседей
            тоже из минорного класса $X_2$
            \item Новый объект: \(X = \alpha X_1 + (1 - \alpha)X_2, 
            \alpha \sim U(0, 1)\)
            \item Предполагает существование объектов между $X_1, X_2$
        \end{enumerate}
    \item Аугментации
    \end{enumerate}
\end{enumerate}

\textbf{Одноклассовая классификация}

Бенчмарк: Классификация X на нормальные и аномальные, 
стандартные метрики

\begin{enumerate}
    \item Статистический подход - описываем плотностью $p(x)$ для
    новых объектов смотрим на вероятность - $p(x)$ - novelty score.

    Откуда брать p
    \begin{enumerate}
        \item Параметрический подход:
        
        \[\sum_{i = 1}^l logP(x \mid \theta) 
        \rightarrow \max_{\theta}\]
        \item Непараметрический подход:
        \begin{enumerate}
            \item d = 1:
            
            \[p(x) = \lim_{h \to 0} 
            \frac{1}{2h}P(\xi \in [x - h, x + h])\]

            \[\hat{p}(x) = \frac{1}{2h}\frac{1}{l} 
            \sum_{i = 1}^l \mid [x_i - x \mid < h] =\]

            \[= \frac{1}{lh} \sum_{i = 1}^l 
            \frac{1}{2}[\frac{\mid x_i - x \mid}{h} < 1]\]

            Можно заменить на более гладкую плотность:

            \[\frac{1}{lh} \sum_{i = 1}^l 
            \frac{1}{2}K(\frac{x_i - x}{h})]\]

            \begin{enumerate}
                \item $K(z) = K(-z)$
                \item $\int_{\mathcal{R}} K(z)dz = 1$
                \item $K(z) \geq 0$
                \item Не возрастает при $Z > 0$
            \end{enumerate}
            \item d > 1:
            
            \[\hat{p}(x) = \frac{1}{lV(h)} \sum_{i = 1}^l 
            K(\frac{\rho(x_i, x)}{h})\]

            \[V(h) = \int K(\frac{\rho(x_i, x)}{h}) dx\]

            h - гиперпараметр
        \end{enumerate}
    \end{enumerate}

    \item Метрический подход
    
    x - аномалия, если он далеко от других объектов

    Смотреть на количество объектов в $\epsilon$-окрестности?

    Плохой подход:

    Надо смотреть не на единую окрестность, а смотреть на 
    плотность объектов в отдельной точке и на основе нее 
    оценивать окрестность

    \textbf{Определения:}

    \begin{enumerate}
        \item $\rho_k(x)$ - такое минимальное число n, что:
        
        Для $\geq$ k объектов из $X/\{x\}$
        выполнено $\rho(x, z) \leq n$

        Для $\leq$ k-1 объектов выполнено $\rho(x, z) < n$

        По сути: расстоение до k-го ближайшего соседа

        \item К-окрестность: 
        
        \[\mathcal{N}_{k}(x) = \{z \in X/\{x\}\}: 
        \rho(x, z) \leq \rho_k(x)\]

        \item Reachibility Distance:
        
        \[rd_k(x, z) = max(\rho_k(z), \rho(x, z))\]

        Позволяет сгладить расстояние между объектами

        \item Local Reachibility Distance
        
        \[lrd_k(x) = \frac{1}{\frac{1}{\mid\mathcal{N}_{k}(x)\mid}\
        \sum_{z \in \mathcal{N}_{k}(x)}rd_k(x, z)}\]

        Обращенное среднее расстояние от x до ближайших соседей

        \item Local Outlier Factor
        
        \[LOF_k(x) = \frac{\frac{1}{\mid\mathcal{N}_{k}(x)\mid}\
        \sum_{z \in \mathcal{N}_{k}(x)}lrd_k(z)}
        {lrd_k(x)}\]

        Отлавливаем объекты у которых соседи находятся в плотных
        областях, но сами они находятся далеко от соседей
    \end{enumerate}

    \item Model-based AD
    \begin{enumerate}
        \item Есть примеры нормальных объектов
        \item Хотим найти наименьшую область,
        содержащую все объекты

        \[a(x) = sign(<w, x> - \rho)\]

        Идея:

        Отделяем X от начала координат с помощью a()

        \[\begin{cases}
            \frac{1}{2}\mid\mid w \mid\mid^2 + 
            \frac{1}{\nu \ell}\sum \xi_i - \rho
            \rightarrow \min_{w, \xi, \rho} \\
            <w, x_i> \geq \rho - \xi_i, \forall i \\
            \xi_i \geq 0
        \end{cases}\]
        
        $\nu$ - гиперпараметр

        $\sum [a(x) = -1] \leq \nu$

        Требования к решению:

        \begin{enumerate}
            \item Отделить как можно больше объектов от 0.
            За это отвечает $\sum \xi_i$
            \item Максимизировать отступ. 
            За это отвечает $\mid\mid w \mid\mid^{2}$
            \item Гиперплоскость как можно дальше от нуля.
            За это отвечает $\rho$
        \end{enumerate}

        \[a(x) = sign(<w, x> - \rho)\]

        Можно записать двойственную задачу:

        \[
        \begin{cases}
            \frac{1}{2}\sum \lambda_i\lambda_jK(x_i, x_j)
            \rightarrow \min_{\lambda} \\
            0 < \lambda_i \leq \frac{1}{\nu \ell} \\
            \sum \lambda_i = 1
        \end{cases}    
        \]

        С помощью ядра получаем компактную область
    \end{enumerate}

    \item Random projections
    \begin{enumerate}
        \item Isolation Forest
        
        Строим жадное дерево со случайными предикатами по
        случайным признакам

        Если в каком-то листе оказывается 1 объект - прекращаем
        разбиение

        Аномальные объекты рано получают свой лист

        \textit{Обучение:}

        Строим лес из N деревьев, в каждом 
        случайные предикаты. Максимальная глубина
        $D = log_2 \ell$

        \textit{Применение:}

        $h_n(x)$ - оценка аномальности x с точки зрения
        n дерева

        $K_n(x)$ - глубина листа в который попадает x в 
        n дереве

        Нужно сделать поправку на количество объектов в листе

        \[h_n(x) = K_n(x) + C(m_n(x))\]

        \[C(m) = 2H(m - 1) - 2\frac{m - 1}{m}\]

        \[H(i) \approx lni + 0.577\]

        Можно использовать и $log_2(m)$

        \[a(x) = 2^{-\frac{\frac{1}{N}\sum_{n = 1}^{N}h_n(x)}
        {C(l)}}\]

        $C(l)$ - средняя длина пути

        \item Extra Random Trees
        
        Берем индикатор попадания в листья 
        и строим линейную модель
    \end{enumerate}
\end{enumerate}

\textit{Как измерять качество:}

\begin{enumerate}
    \item Anomaly detection
    
    Есть примеры аномалий, но мало данных

    Смотрим какое количество аномалий модель угадывает

    \item Novelty detection
    
    Аномалии не даны, 
    качество модели оценивается глазами
\end{enumerate}

\section{Обучение без учителя}

\begin{enumerate}
    \item Кластеризация: DBScan, 
    Спектральная классификация,
    Affinity Propagation
    \item Внешние метрики качества кластеризации
    \item Тематическое моделирование
\end{enumerate}

\textbf{K-means:}

Основная проблема - ищет сферические кластеры

\subsection{DBScan}

Типы объектов:

\begin{enumerate}
    \item Ядровые:
    
    В $\epsilon$-окрестности находится N объектов

    \item Пограниченые объекты:
    
    Достижимы из ядровых, находится в 
    $\epsilon$-окрестности ядрового

    \item Выбросы:
    
    Все остальные
\end{enumerate}

Псевдокод:

\begin{lstlisting}
    K = 0 #Num clasters
    rho #metric
    epsilon, N #hyperparam
    for i = 1 ... l:
        if label(x[i]) != 0:
            continue
        #point neighborhood
        U = {x in X | rho(x[i], x[j] <= epsilon)}
        if |U| < N:
            label(x[i]) = noise
            continue
        K++ # found new claster if |U| > N
        label(x[i]) = K
        U = U \ {x[i]}
        for x[j] in U:
            if label(x[j]) = noise:
                label(x[j]) = K
            if label(x[j]) != 0:
                continue
            label(x[j]) = K
            #point neighborhood
            R = {xm in X | rho(x[m], x[j]) < epsilon}
            if |R| >= N:
                #new core object neighborhood included in U
                U = U & R
\end{lstlisting}

Преимущества:

\begin{enumerate}
    \item Находит кластеры сложной формы
    \item Находит выбросы
    \item Быстрый
    \item Не надо задавать число кластеров
\end{enumerate}
    
Недостатки:

\begin{enumerate}
    \item Проблемы если кластеры разной плотности
    \item Проблемы с точками на краях
    \item Не работает если кластеры характеризуются неплотность
    \item Не параллелится
\end{enumerate}

\subsection{Иерархическая кластеризация}

Цель: Найти кластерную структуру

Визуализировать разную структуру кластеров при разном 
их количестве

\textit{Агломеративная кластеризация}

Начинаем с того, что каждый объект является кластером

Псевдокод:

\begin{lstlisting}
    C = {{x[1]}, {x[2]}, ..., {x[l]}}
    for n = 2, ..., l:
        G, H = argmin rho(G, H) #find nearest clasters
        C = (C \ {G, H}) U {G U H}    
\end{lstlisting}

Функция расстояния между кластерами $\rho$:

\begin{enumerate}
    \item Single Linkage:
    \[\rho_{sl}(G, H) = 
    \min_{x_i \in G, x_j \in H}\rho(x_i, x_j)\]

    Чувствителен к выбросам

    Главная проблема: Chaining

    Алгоритм подцепляет отдельные объекты, а не кластеры

    Дендрограмма - картинка присоединения объектов

    \item Complete linkage
    
    \[\rho_{cl}(G, H) = 
    \max_{x_i \in G, x_j \in H}\rho(x_i, x_j)\]

    Кластеры не будут компактными

    \item Group Average
    
    \[\rho_{ga}(G, H) = \frac{1}{\mid G \mid \mid H \mid} 
    \sum_{x_i \in G, x_j \in H}\rho(x_i, x_j)\]
\end{enumerate}

\subsection{Графовая кластеризация}

G = (X, E)

E - ребра:

\begin{enumerate}
    \item Полный граф - все вершины связаны
     
    $w_{ij} = exp(-\frac{\norm{x_i - x_j}^2}{2 \sigma^{2}})$
    \item KNN-граф:
    
    $w_ij \neq 0 \Leftrightarrow x_i, x_j$ ближайшие соседи
    
    \item $\epsilon$-граф:
    
    $w_ij \neq 0 \Leftrightarrow \rho(x_i, x_j) \leq \epsilon$ 
\end{enumerate}

\textit{Поиск решение}

\begin{enumerate}
    \item Найти связные компоненты (для 3его варианта)
    
    Тупой метод
    \item Минимальное остовное дерево (Алгоритм Краскала)
    \begin{enumerate}
        \item Начинаем с отдельных вершин
        \item Сливаем два кластера с максимальным ребром между ними
        \item Повторить пока не будет K кластеров
        \item Это агломеративная кластеризация с sl
        \item Решает задачу:
        
        \[\max \min_{x_i \in G, x_j in H} \rho(x_i, x_j)\]
    \end{enumerate}
    \item Спектральная кластеризация
    
    \[A, B \subset X, A \cap B = \emptyset\]
    \[W(A, B) = \sum_{x_i \in A, x_j \in B} w_ij\]
    \[X = A_1 \cup A_2 \cup \ldots A_k\]

    Ошибка кластеризации:

    \[\textrm{Ratio Cut}(A_1, \ldots, A_k) = 
    \frac{1}{2}\sum_{i = 1}^K \frac{w(A_i, \bar{A}_i)}
    {\mid A_i \mid} \rightarrow \min_{A_i, \ldots, A_k} (\star)\]
    \[\bar{A}_i = X \setminus A_i\]

    Хотим, чтобы ребра между кластерами были как можно менее
    значимы $\rightarrow$ Каждый кластер должен быть изолированным

    $K = 2 \rightarrow$ Задача поиска максимального потока

    $K > 2 \rightarrow$ NP-полная задача

    \rule{\linewidth}{0.5pt}

    $G = (X, E)$
    
    $d_i = \sum_{j = 1}{d} w_ij$ - сумма ребер, 
    которые с ней связаны

    $D = diag(d_1, \ldots, d_l)$

    $L = D - W$, W - матрица смежности, L - \textbf{лаплассиан}

    \textit{Свойства лаплассиана}

    \begin{enumerate}
        \item \[f \in R^{d}\]
        \[f^TLf = \frac{1}{2}\sum_{i, j = 1}^l 
        w_{ij}(f_i - f_j)^2\]
        \item L - симметричная
        \item L - неотрицательно определенная
    \end{enumerate}

    \textbf{Теорема}

    \begin{enumerate}
        \item Кратность $\lambda = 0$ у L равна
        числу компонент связности графа
        
        Кратность:
        
        \begin{enumerate}
            \item Собственные значения: $Ax = \lambda x$
            \item $det(A - \lambda I) = 0 \rightarrow \lambda_i$
            \item $\lambda_i$ - решениe
            \item ${\lambda_i}, \forall i$ - спектр графа
            \item Характеристическое уравнение выражаем в виде
            характеристического многочлена и раскладываем его
            в виде решений

            \[P_A(\lambda) = 
            (-1)^{n}\prod_{i = 1}^{n}(\lambda - \lambda_i) = 
            (-1)^n(\lambda - \lambda_1)^{k_1}\ldots\]

            $k_1$ называется кратностью для $\lambda_1$
        \end{enumerate}
        \item $A_1, \ldots, A_k$
        
        Вектор индикатор: $f_i = ([x_j \in A_i])_{j = 1}^{\ell}$

        $f_1, \ldots, f_k$ - собственные векторы для $\lambda = 0$
    \end{enumerate}

    \underline{Доказательство:}

    K = 1:

    \begin{enumerate}
        \item Является ли $\lambda = 0$ собственным значением
        \[f = (1, \ldots, 1)\]
        \[Lf = Df - Wf =
        \begin{pmatrix}
            d_1 \\
            \vdots \\
            d_{\ell}
        \end{pmatrix} - 
        \begin{pmatrix}
            \sum w_{1, j} \\
            \vdots \\
            \sum w_{\ell, j}
        \end{pmatrix} = 0\]
        \item Кратность $\lambda = 0$ = 1 $\rightarrow$
        нет других собственных векторов
        
        Допустим:
        \[\exists f^{\prime} \in R^{\ell}: 
        \exists p \neq q,
        f^{\prime}p \neq f^{\prime}q, 
        Lf^{\prime} = 0\]

        \[(f')^TLf' = \frac{1}{2}\sum_{i, j = 1}^{\ell}
        w_{ij}(f_i' - f_j')^2 = 0\]
        \[\forall i,j: 
        \bcases{w_{ij} = 0 \textrm{ Нет ребра} \cr 
        f_i' = f_j'}\]

        Граф G связный $\rightarrow$ 
        Существует путь из p в q $\rightarrow$

        Путь: $w \rightarrow i_1 \rightarrow 
        \ldots \rightarrow p$

        $w_{pi_1} \neq 0 \rightarrow f'p = f'_{i_1}
        \rightarrow w_{i_1\ldots} \neq 0 \rightarrow \ldots
        \rightarrow f'_p = f'_{i_1} = \ldots = f'_q$

        \[\bot\]
    \end{enumerate}

    K > 1:

    Можно упорядочить объекты так, чтобы L был
    блочно-диагональным
    
    \[L = 
    \begin{pmatrix}
        L_1 & 0 & 0 \\
        0 & L_2 & 0 \\
        & \ddots & \\
        0 & 0 & L_K
    \end{pmatrix}\]

    Спектр блочно-диагональной матрицы = 
    объединение спектров отдельных блоков

    $L_i \rightarrow f_i = ([x_j \in A_i])_{j = 1}^{\ell}$

    Кратность $\lambda = 0$ равна K

    \rule{\linewidth}{0.5pt}

    Гипотеза: $x_i, x_j$ - похожие объекты $\Rightarrow$
    у собственного вектора $f_i$ для $\lambda_i \approx 0$,
    будет $f_ij \approx f_ik$

    Для связанных графов не берем $\lambda = 0$, 
    т.к. тогда будет одна компонетна

    \textbf{Алгоритм:}
    \begin{enumerate}
        \item Строим лаплассиан
        \item Находим m (гиперпараметр) нормированных 
        собственных векторов $u_1, \ldots, u_m$,
        соотв. наименьшим собственным значениям. 
        
        Сложность: $O(l^3)$
        \item $U = (u_1 \mid \ldots \mid u_m) \in R^{\ell \times m}$
        \item Новые признаки близки для объектов 
        в одной плотной области
        \item K-means
    \end{enumerate}

    \textbf{Как это связано с задачей $(\star)$:}

    Если эту задачу релаксировать и 
    искать не жесткое приписывание к классам, а распределение,
    то ее решение U.
\end{enumerate}

\section{Метрики качества классификации, 
тематическое моделирование}

\subsection{Affinity Propagation}

Цель - найти типовые объекты и на их основе выделять кластеры

Сходство вершин:

\[s(i, k) = -\norm{x_i - x_k}^2\]

$r(i, k)$ - Насколько $x_k$ является типовым объектом для $x_i$

$a(i, k)$ - Насколько у $x_i$ важный голос для типового объекта

Инициализируем 0 и итеративно рассчитываем показатели:

\[r(i, k) = s(i, k) - \max_{k' \neq k}(a(i, k') + s(i, k'))\]

Если рядом есть более близкие объекты, чем k, 
то k не очень хороший представитель.

\[a(i, k) = \min(0, r(k, k) + \sum_{i' \neq i, i' \neq k}max(r(i', u)))\]

\[\alpha(x_i) = \argmax_{k \in \{1, \ldots, \ell\}}(r(i, k) + a(i, k))\]

\subsection{Оценка качества кластеризации}

\begin{enumerate}
    \item Внутренние
        \begin{enumerate}
            \item Без использование лейблов
            \item Внутрикластерное расстояние
            \item Межкластерное расстояние
        \end{enumerate}
    \item Внешние
    \begin{enumerate}
        \item Знаем истинные номера кластеров $y_1, \ldots, y_{\ell}$
        \item Номера кластеров нельзя сравнивать с истинными
        \item Посчитать K! перестановок и найти классы?
        \item \textbf{Требования к метрике}
        \begin{enumerate}
            \item Гомогенность
            
            Значение метрики качества должно уменьшаться при 
            объединении в один кластер двух эталонных

            \item Полнота
            
            Значение метрики качества должно уменьшаться 
            при разделении эталонного кластера на части

            \item Rag bag
            
            Значение метрики качества должно быть выше у той версии кластеризации, 
            которая помещает новый нерелевантный обоим кластерам элемент в шумный кластер, 
            по сравнению с версией, которая помещает этот элемент в чистый кластер

            \item Cluster size vs quantity
            
            Значительное ухудшение кластеризации большого числа небольших кластеров должно обходиться дороже 
            небольшого ухудшения кластеризации в крупном кластере.
        \end{enumerate}
        \item \textbf{Метрики}
        \begin{enumerate}
            \item Bcubed
            
            $y(x)$ - номер кластера в истинной разметке
            
            $a(x)$ - выход кластеризации

            Correctness:

            \[C(x_i, x_j) = 
            \begin{cases}
                1, y(x_i) = y(x_j) & a(x_i) = a(x_j) \\
                0, otw
            \end{cases}\]

            \[\textrm{Precision-Bcubed} = Avg_{x_i}(Avg_{x_j, a(x_i) = a(x_j)}(C(x_i, x_j)))\]

            \[\textrm{Recall-Bcubed} = Avg_{x_i}(Avg_{x_j, y(x_j) = y(x_i)} C(x_i, x_j))\]

            \[F_{\textrm{Bcubed}} = 2 \frac{\textrm{Precision}\textrm{Recall}}{\textrm{Precision} + \textrm{Recall}}\]
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\subsection{Подбор метрик для продукта}

Вводим набор ухудшений 

\section{Тематическое моделирование}

Методы кластеризации для текстов

Есть $T$ тематик

$x_d$ - документ

$\theta_d \in R^T$ - распределение тематик для документа

$\phi_t in R^W$ - тема описывается распределением на словах

$W$ размер словаря

\subsection{LSA (Latent Semantic Analysis)}

\[X \in R^{d \times W}\]

$X_{dw}$ - сколько раз слово w входит в документ d

\[X = \Theta \times \Phi, \Theta \in R^{d \times T}, \Phi in R^{T \times W}\]

\[x_{dw} = \sum_{t = 1}^{T} \theta_{dt} \times \phi_{wt}\]

Делаем SVD для разложения 

\subsection{PLSA}

Хотим ввести вероятности

\[p(t \mid d) = \theta_{td}\]
\[p(w \mid t) = \phi_{wt}\]

Генерация текста $x_d$

\begin{enumerate}
    \item Сэмплируем тему $t \sim p(t \mid d)$
    \item Сэмплируем слово $w \sim p(w \mid t)$
    \item Добавляем слово в текст
    \item Повторяем до нужной длины
\end{enumerate}

$\theta_{dt}, \phi_{tw}$ - параметры модели

Неполное правдоподобие данных:

\[\begin{cases}
    \sum_{d = 1}^D \sum_{t = 1}^T \sum_{w = 1}^W[t_{dj} = t]log\phi_{w_{d_jt}}\theta_{td} \\
    \theta_{td}, \phi_{wt} \geq 0 \\
    \sum \theta, \sum \phi = 1
\end{cases}
\]

Тема является скрытой переменной

Можно построить ЕМ-алгоритм по этой задаче:

Е-шаг: $p(t_{dj} \mid d, w_{dj})$

М-шаг: $\phi_{wt}, \theta_{td} = ?$

\subsection{LDA (Latent Dirichlet Allocation)}

В PLSA не требуем невырожденности распределений

Дополнительно вводим распределения параметров:

$\Phi_t = Dir(\alpha)$

$\Theta_d = Dir(\beta)$

\[Dir(x_1, \ldots, x_n, \alpha) = \frac{\Gamma (\alpha n)}{\Gamma^n(\alpha)}\prod_{i = 1}^n x_i^{\alpha - 1}\]

Распределение на дискретных распределениях с n исходами

\section{Частичное обучение (semisupervised)}

Используется в случае:

Есть обучающая выборка $X^l = \{(x_i, y_i)\}_{i = 1}^{\ell}$

Есть неразмеченная выборка: $X^u = \{(x_i)\}_{i = \ell + 1}^{n}$

Самая большая ценность состоит в необычных объектах

Мотивация: Неразмеченные данные собрать проще, чем размеченные

Категории задачи:

\begin{enumerate}
    \item Semisupervised learning: $X_{\ell} \cup X_u \rightarrow a(x)$
    \item Transductive learning $X_{\ell} \cup X_u \rightarrow$ Найти метки для $X_u$
\end{enumerate}

\textbf{Методы:}

\begin{enumerate}
    \item Self-training
    
    \begin{enumerate}
        \item Обучить a(x) на $X^{\ell}$
        \item Применить на $X^u$
        \item Добавить $(x_i, a(x_i))$ в $X^{\ell}$ объекты на 
        которых модель наиболее уверена

        Критерий: 
        \begin{enumerate}
            \item Для классификации: самые большие уверенности
            \item Брать по порогу расстояния к обучающим
            \item Всю $X^u$ с весами на основе уверенности модели
            
            Взвешиваем лосс
        \end{enumerate}

        \item Повторить
    \end{enumerate}
    \item Генеративные модели
    
    Описываем каждый класс нормальный распределением

    Если бы были только $X^{\ell}$:

    Правдоподобие и максимизация по параметрам:

    \[\sum_{i = 1}^{\ell} logP(y_i \mid \theta)P(x_i \mid y_i, \theta) \rightarrow \max_{\theta}\]

    Если $X^{\ell}$ и $X^{u}$:

    Неразмеченные данные хотим описать как смесь распределений классов

    \[\sum_{i = 1}^{\ell} logP(y_i \mid \theta)P(x_i \mid y_i, \theta) +
    \sum_{i = \ell + 1}^n log \sum_{y = 1}^K p(y \mid \theta)p(x_i \mid y, \theta) 
    \rightarrow \max_{\theta}\]

    Используем EM-алгоритм для поиска $\theta$

    Второе слагаемое может перевесить - нужно добавить $\lambda$

    \item Упрощенная версия: Cluster-and-label:
    
    Обучаем алгоритм кластеризации и помечаем в каждом кластере
    объекты самым популярным классом

    \item Методы на основе моделей
    
    \begin{enumerate}
        \item Логит $\rightarrow$ Expectation Regularization
        \item Semi-Supervised SVM = S3VM
        
        Безусловная задача оптимизации SVM:

        \[\norm{w}^2 + C\sum_{i = 1}^{\ell}\max(0, 1 - y_i<w, x_i>) 
        \rightarrow \min_{w}\]

        Лосс > 0 когда расстояние до гиперплоскости отрицательное

        Для unsupervised: чего требовать?

        Если объект близко к гиперплоскости - штрафуем, 
        если далеко - не штрафуем
        
        Задача:

        \[\norm{w}^2 + C_1 \sum_{i = 1}^{\ell} \max(0, 1 - y_i<w, x_i>)
        + C_2 \sum_{i = \ell + 1}^n \max(0, 1 - \mid<w, x_i>\mid)
        \rightarrow \min_{w}\]

        Может подобрать гиперплоскость, которая просто лежит
        далеко от данных и не разделяет их

        Можно потребовать, чтобы баланс классов был такой же,
        как и на размеченных данных

        \[\frac{1}{n - \ell} \sum_{i = \ell + 1}^n a(x_i)
        = \frac{1}{\ell} \sum_{i = 1}^{\ell} y_i\]

        Тогда гиперплоскость не может просто не разбивать классы

        Очень сложная задача с точки зрения оптимизации- 
        максимум, модуль, ограничения

        \textbf{CCCP: ConCave Convex Procedure}
        
        Метод для оптимизации суммы выпуклой и вогнутой функции
    \end{enumerate}
    \item Графовые методы
    
    \[a(x) = \infty \sum_{i = 1}^{\ell} (a(x_i) - y_i)^2 +
    \sum_{i, j = 1}^n w_{ij}(a(x_i) - a(x_j))\]

    \[w_{ij} = exp \left(- \frac{\norm{x_i - x_j}^2}
    {2\sigma^2}\right)\]

    Согласованные метки на соседних объектах для неразмеченных

    Бесконечно более важно оптимизировать первый элемент

    Упрощение задачи (Manifold Regularization):

    \[\sum_{i = 1}^{\ell} L(y_i, a(x_i)) +
    \lambda_1 R(a) + 
    \lambda_2 \sum_{i, j = 1}^n w_{ij}(a(x_i) - a(x_j))\]

    \[\sum_{i, j = 1}^n w_{ij}(a(x_i) - a(x_j)) = a^T L a\]
\end{enumerate}

\section{Метрические методы}

Case-based reasoning - не очень зависит от параметров

$\rho: X \times X \rightarrow (0, + \infty)$ - функция расстояния

Обучение: запоминаем X

Применение:

u - новый объект

Строим вариационный ряд: 

\[\rho(u, x_1) \leq \rho(u, x_2) \ldots\]

Классификация:

\[a(u) = \argmax_{y \in Y} \sum_{k = 1}^K w(K, u, x_k) [y_k = y]\]

Веса учитывают расстояния до точки:

\[w(K, u, x_k) = K(\frac{\rho(u, x_k)}{h})\]

Регрессия (Формула Надарая-Ватсона):

\[a(u) = \frac{\sum_{k = 1}^K w y_k}{\sum_{k = 1}^K w}\]

Зачем нужен kNN?

\begin{enumerate}
    \item Если легко задать расстояния, 
    но сложно придумать признаки
    \item Если задача решается через сходство
    \item Мало представителей каждого класса
\end{enumerate}

Оптимальность kNN:

\begin{align*}
    & Y = \{-1, +1\} \\
    & p(y = +1 \mid x) \\
    & u \textrm{ - хотим классифицировать} \\
    & x_u \textrm{ - ближайший сосед} \\
    & p_{bayes}^* \textrm{ - вероятность ошибки 
    опт. Байесовского класс} \\
    & p_{1nn} \leq 2p_{bayes}, if l \to \infty
\end{align*}

\rule{\linewidth}{0.5pt}

\textbf{Пример хитрой метрики}

Хотим сделать расстояние на текстах

$C(i, j)$ - расстояние между представлениями i и j слов

$t_{ij}$ - количество смысла, перетекающего из $x_i$ в $z_j$

$x_i$ - число вхождений некоторого слова i из словаря в текст x

$\sum_{j = 1}^d t_{ij} = x_i$

$\sum_{i = 1}^d t_{ij} = z_j$

$t_{ij} \geq 0$

Стоимость переноса смысла

$\sum_{i, j = 1}^d t_{ij}C(i, j) = \mu(x, z) 
\rightarrow min_{t_{ij}}$

Для оптимального $t_{ij}$: $\mu(x, z) = \rho(x, z)$

Задача оптимизации: min cost max flow

\rule{\linewidth}{0.5pt}

\subsection{Быстрый поисх ближайших соседей}

Зачем?

\begin{enumerate}
    \item Задачи retrieval
    \item Рекомендации
    \item \dots
\end{enumerate}

\subsubsection{Точные методы}

\begin{enumerate}
    \item KD-деревья и другие структуры
    
    При росте d сложность приближается к линейной
\end{enumerate}

\subsubsection{Приближенные решения}

\begin{enumerate}
    \item LSH - locality sensitive hashing
    \begin{enumerate}
        \item \underline{Определение:}
        
        Семейство функций $\mathcal{F} = \{f: X \to H\}$
        с распределением P(f) называется $(d_1, d_2, p_1, p_2)$
        чувствительным, если

        \begin{enumerate}
            \item $p(x, z) \leq d_1 \Rightarrow 
            P_{f \in \mathcal{F}}[f(x) = f(z)] \geq p_1$
            \item $p(x, z) \geq d_1 \Rightarrow 
            P_{f \in \mathcal{F}}[f(x) = f(z)] \leq p_2$
        \end{enumerate}

        \rule{\linewidth}{0.5pt}
        
        Пример: MinHash

        Объекты - множества, $x \subset U = \{u_1, \ldots, u_n\}$

        $\pi$ - перестановка на множестве U

        $f_{\pi}(x) = \min \{\pi(i) \mid u_i \in A\}$

        \underline{Утверждение:}

        \[P_{\pi}[f_{\pi}(A) = f_{\pi}(B)] = 
        \frac{\mid A \cap B \mid}{\mid A \cup B \mid}\]

        \underline{Док-во:}

        Три категории $u \in U$:

        \begin{enumerate}
            \item $u \in A, u \in B$
            \item $u \in A, u \not{\in} B$
            
            или

            $u \not{\in} A, u \in B$
            \item $u \not{\in} A, u \not{\in} B$
            
            $\pi = 
            \begin{pmatrix}
                u_1 \\
                u_2 \\
                u_3 \\
            \end{pmatrix}$

            Одинаковые хэши для A и B?

            $u$ из первой группы должны иметь меньший хэш

            Какова доля перестановок, где хотя бы 1 элемент
            первого типа идет раньше всех второго типа:

            $\frac{p}{p + q} = 
            \frac{\mid A \cap B \mid}{\mid A \cup B \mid}$

            $\rho(A, B) = 1 - \frac{\mid A \cap B \mid}
            {\mid A \cup B \mid}$

            $\rho(A, B) \leq d_1 \Rightarrow 
            P_{f \in \mathcal{F}}[f(x) = f(z)] =  \frac{\mid A \cap B \mid}
            {\mid A \cup B \mid} = 1 - \rho(A, B) \geq 1 - d_1$

            $\rho(A, B) \geq d_2 \Rightarrow P \leq 1 - d_1$

            MinHash - $(d_1, d_2, 1 - d_1, 1 - d_2)$ чувствителен

        \end{enumerate}

        \rule{\linewidth}{0.5pt}

        \item Для косинусного расстояния:
        
        \(\rho(x, y) = \arccos \frac{<x, y>}{\norm{x}\norm{y}}\)

        \(\mathcal{F} = \{f_w(x) = sign<w, x> \mid w \in R^d\}\)

        Геометрически: 
        Накидываем случайные гиперплоскости w и смотрим с какой
        стороны от них находятся точки

        Если между точками угла маленький - вероятность их 
        рассечения плоскостью мала

        \item Для евклидова расстояния:
        
        \(\mathcal{F} = \{f_{w, b}(x) = \frac{<w, x> + b}{r} 
        \mid w \in R^d, b \in [0, r)\}\)

        Геометрически: проводим случайную прямую,
        разбиваем на отрезки длины r. Значение хэш функции -
        номер отрезка

        \(w \sim \mathcal{N}(0, I)\)
        \(b \sim U[0, r)\)

        Если варьировать распределения, то можно получить
        другие метрики:

        Метрики Минковского - $\rho \in (0, 2]$

        Манхэттенская метрика ($\rho = 1$): $w \sim Cauchy$

        \item Проблема этих методов
        \begin{enumerate}
            \item Не очень устойчиво из-за вероятностного подхода
            \item При большом расстоянии все равно есть вероятность
            совпадения хэшей
            \item Хотим форму сигмоиды, а не линейно 
            убывающую вероятность равенства хэшей
        \end{enumerate}

    \item Композиция хэш-функций:
    
    \(g(x) = (f_1(x), \ldots, f_m(x))\)

    Алгоритм: $x \rightarrow g(x) \rightarrow 
    \{x_i \in X \mid g(x_i) = g(x)\} \rightarrow
    $ Ищем ближайших соседей среди N(x)

    \(d = \rho(x, z)\)

    \(P(f_1(x) = f_1(z)) = p\)

    \(P(g(x) = g(z)) = p^m\)

    Степенная функция это не то, что нам нужно

    Модифицируем:

    \(g_1(x) = (f_{11}(x), \ldots, f_{1m}(x))\)
    
    \(\vdots\)

    \(g_L(x) = (f_{L1}(x), \ldots, f_{Lm}(x))\)

    \(P[[g_1(x) = g_1(z)] \mid \mid \ldots 
    \mid \mid [g_L(x) = g_L(z)]] = 1 - (1 - p^m)^L\)

    \item Теория:
    
    Алгоритм решает задачу поиска c-ближайшего соседа,
    если для нового объекта u с вероятностью $1 - \epsilon$,
    алгоритм возвращает объект выборки $z \in X: \rho(u, z) 
    \leq C\rho(u, x_{\star})$, где $x_{\star}$ - ближайший сосед u

    Для LSH:

    \(\exists L, m: O(d \ell^n log \ell)\) - время поиска в LSH
    \end{enumerate}

    \item \textbf{NSW} (Navigable Small World)
    
    \begin{enumerate}
        \item Small world graph -
        если сгенерировать случайный граф - среднее расстояние
        между двумя парами вершин очень близко
        \item Представляем выборку в виде графа,
        где у каждой вершины небольшая степень,
        но выполняется свойство малого мира

        \item \(G = (X, E)\)
        
        Задаем алгоритм жадного поиска:
        \begin{enumerate}
            \item u - новый объект
            \item Берем случайную вершину v в G
            \item В цикле: Среди всех соседей v ищем
            вершину $v^{\prime}: \rho(v^{\prime}, u) < \rho(v, u)$
            \item Если такой сосед нашелся - переходим в него
            \item \underline{Используем мультистарт}
            
            Находим множество результатов $C_u$, можно расширить 
            это множестве окрестностями $C_u$ - выбираем ближайшие
        \end{enumerate}

        \item Добавление вершины u:
        \begin{enumerate}
            \item Мультистарт - $C(u)$
            \item $D(u) = C(u) \cup $ окрестности вершины
            \item Соединяем u с k ближайшими соседями из D(u)
        \end{enumerate}

        \item Особенность метода:
        \begin{enumerate}
            \item В графе есть области плотности и
            связующие цепочки
            \item Свойство малого мира достигается за счет
            связующих цепочек - вершин с очень высокой степенью
        \end{enumerate}
    \end{enumerate}
    \item \textbf{HNSW} (hierarchical NSW):
    \begin{enumerate}
        \item На следующий уровень пропускам только log от числа
        вершин с некотрой вероятностью
        \item Начинаем с самого верхнего уровня графа: 
        находим ближайшего соседа
        \item Начинаем из этой точки на более низком уровне,
        ищем новую точку
        \item \dots
    \end{enumerate}
\end{enumerate}

\section{Задача ранжирования}

\(X = \{x_1, \ldots, x_l\} \subset X\)

\((i, j) \in R \subset \{1, \ldots, \ell\}^2 \Rightarrow
a(x_i) > a(x_j)\)

$\{1, \ldots, \ell\}^2$ - множество всех пар

Обычно так:

\(x = (q, d)\) - q - запрос, документ

в R - пары $x_i = (q_i, d_i), x_j = (q_j, d_j)$, где $q_i = q_j$

\underline{Метрики качества ранжирования}

\begin{enumerate}
    \item Наивный подход:
    
    \begin{enumerate}
        \item \(R \rightarrow y_1, \ldots, y_{\ell}: 
        (i, j) \in R \Rightarrow y_i > y_j\)
        \item Обучаем $a(x_i) \approx y_i$, 
        метрика - точность прогнозов
        \item Модель может правильно ранжировать, но при этом
        выдавать лейблы далекие от исходных и качество будет 
        плохим при хорошем ранжировании
    \end{enumerate}
    
    \item Дефектные пары:
    
    \(\frac{1}{\mid R \mid}\sum_{(i, j) \in R}[a(x_i) \leq a(x_j)]\)
    
    \item precision@k:
    
    Работает, когда таргет является разметкой релевантности
    документов под запрос $y \in \{0, 1\}$

    \(\sum_{i = 1}^K[y(i) = 1]\)

    Проблема:

    Не учитывает порядок выдачи в топ k документов

    \item Average Precision@k(q)
    
    \(AP@k = \sum_{i = 1}^K \frac{y_i}{\sum_{j = 1}^K y_j} \times 
    precision@i\)

    \item MAP@k
    
    Q - множество запросов

    \(MAP@k = \frac{1}{\mid Q \mid}\sum_{q \in Q}AP@k(q)\)

    \item DCG@k (для небинарных меток)
    
    \(DCG@k(q) = \sum_{i = 1}^K g(y_i)d(i)\)

    \(g(y)  2^y - 1\)
    
    \(d(i) = \frac{1}{log(i + 1)}\)

    \item nDCG@k(q)
    
    \(nDCG@k(q) = \frac{DCG@k(q)}{max DCG@k(q)}\)

    \item Каскадные метрики

    pFound:

    Пытается промоделировать поведение пользователей

    $y \in [0, 1]$ - вероятность найти ответ в документе

    $p_i$ - вероятность, что пользователь дойдет до i-ой
    позиции в выдаче

    $p_i = 1$

    $p_{i + 1} = p_i(1 - y_i)(1 - p_{out})$

    $p_out$ - вероятность, что пользователь уйдет

    $pFound@k(q) = \sum_{i = 1}^K p_i \times y_i$
\end{enumerate}

\underline{Признаки для моделей ранжирования}

\begin{enumerate}
    \item Запросные признаки
    
    \begin{enumerate}
        \item Эмбеддинг запроса
        \item Популярность
        \item Категория
        \item Персонализация
        \item Признаки про пользователя
    \end{enumerate}

    \item Статические - только про документ
    
    \begin{enumerate}
        \item Эмбеддинг документа
        \item Категория документа
        \item PageRank
        
        $PR(d) = \frac{1 - \delta}{\mid D \mid} + 
        \delta \sum_{c \in D_{d}^{in}} \frac{PR(c)}
        {\mid D_c^{out} \mid}$ 

        $D_d^{in}$ - мн-во документов, ссылающихся на d

        $D_c^{out}$ - мн-во док., на которые ссылается c
    \end{enumerate}

    \item Динамические признаки - про пару/запрос документ
    
    \begin{enumerate}
        \item Косинусное расстояние между эмбеддингами 
        документа и запроса
        \item BM25
        
        $q = q_1, q_2, \ldots, q_n$ - слова

        $BM25(q, d) = \sum_{i = 1}^n IDF(q_i) \times 
        \frac{TF(q_i, d) \times (K_1 + 1)}{TF(q_i, d) + 
        K_1(1 + b + b\frac{\mid D \mid}{n_d})}$
    \end{enumerate}
\end{enumerate}

\underline{Методы ранжирования}

\begin{enumerate}
    \item Pointwise - поточечные методы
    
    \(q: (d_1, y_1), \ldots, (d_{n_q}, y_{n_q})\)

    \(\sum_{q \in Q}\sum_{i = 1}^{n_q} L(y_i, a(q, d_i)) 
    \rightarrow \min_{a}\)

    \item Попарные методы
    
    Главное, чтобы пары были правильно расположены относительно
    друг друга

    \((i, j) \in R \Rightarrow a(x_i) > a(x_j)\)

    \(\frac{1}{\mid R \mid}\sum_{(i, j) \in R}[a(x_i) < a(x_j)]
    rightarrow \min_a\)
    
    \(\frac{1}{\mid R \mid}\sum[a(x_i) - a(x_j) < 0] \leq\)

    \(\leq \frac{1}{\mid R \mid}\sum_{(i, j \in R)}
    \tilde{L}(a(x_i) - a(x_j)) \ rightarrow \min_{a}\)

    \(\frac{1}{\mid R \mid}\sum log(1 + e^{a(x_j) - a(x_i)}) 
    \rightarrow \min_{a}\)

    Частный случай (RankNet):

    \(a(x) = <w, x>\)
    
    \(\tilde{L}(z) = log(1 + e^{-\sigma z})\)
    
    \(w^t = w^{t - 1} + \eta \frac{\sigma}
    {1 + exp(\sigma<x_j - x_i, w>}(x_j - x_i)\)

    Используем метрику

    \(\delta F_{ij}\) - как изменится метрика качества, 
    если поменять \(x_i x_j\) местами

    LambdaRank:

    \(w^t = w^{t - 1} + \eta \frac{\sigma}
    {1 + exp(\sigma<x_j - x_i, w>}\mid \delta F_{ij} \mid(x_j - x_i)\)

    \item Списочные методы
    
    Напрямую оптимизируем метрики качества:

    ListNet

    \begin{align*}
        & q_1, \ldots, q_m \\
        & q: d_1, \ldots, d_{n_q} \\
        & a(q, d_1) = z_1, \ldots, a(q, d_{n_q}) = z_{n_q} \\
        & \frac{1}{m} \sum_{i = 1}^m nDCG@k(q_i) \rightarrow \max \\
        & nDCG@k(q) = \sum_{i = 1}^K \frac{2^y(i)}{log(i + 1)} \\
        & \textrm{Параметры модели зашиты в порядке ранжирования} \\
        & nDCG@k(q, \pi(a)) = \sum_{i = 1}^K \frac{2^{y(\pi(i)}}{log(i + 1)} \downarrow\\
        & E_{\pi}nDCG@k(q, \pi) = \sum_{\pi 
        \in Sym(q_1, \ldots, q_{n_q})}
        p(\pi)nDCG@k(q, \pi) \\
        & \phi \textrm{ - неубывающая строго + функция} \\
        & p_z(\pi) = \prod_{j = 1}^{n_q} \frac{\phi(z_{\pi(j)})}
        {\sum_{k = j}^{n_q}\phi(z_{\pi(k)})}
    \end{align*}
    
    Свойства этого распределения:

    \begin{enumerate}
        \item Распределение на мн-ве всех перестановок $n_q$
        документов
        \item 
        $\pi_i: d_i$ выше $d_j, z_i > z_j$
        
        $\pi^{\prime}:$ как $\pi_i$ только $d_i$ ниже $d_j$
        
        $p_z(\pi) > p_z(\pi^{\prime})$

        \item Максимальную вероятность имеет перестановка 
        с отсортированной по модели выборке
    \end{enumerate}

    Можно посчитать $\frac{\partial p_z(\pi)}{\partial a}$

    Задача: $E_{\pi}nDCG@k(q, \pi) \rightarrow \max_{a}$

    $\sum_{\pi 
    \in Sym(q_1, \ldots, q_{n_q})}
    p(\pi)nDCG@k(q, \pi) \rightarrow \max_a$

    Очень много ($!n_q$) слагаемых

    В ListNet перестановки делаются иначе:

    Вероятность, что j документ попадет на 
    первое место в перестановке:

    $p_z(j) = \frac{\phi(z_j)}{\sum_{i = 1}^n \phi(x_i)}$

    $Q(y, z) = - \sum_{j = 1}^{n_q}p_y(j)logp_z(j) \rightarrow \min_a$
\end{enumerate}

\section{Рекомендательные системы}

Задача, где любые две сущности надо сопоставлять друг с другом

Определения:

Множество пользователей:

$U = \{u_1, \ldots, u_n\}$

Множество айтемов:

$I = \{i_1, \dots, i_m\}$

Для некоторых (u, i) $\exists r_{ui}$

$R = \{(u, i) \mid \exists r_{ui}\}$

Нужно найти $a(u, i)$

$U \xrightarrow{\textrm{Запрос}}$ Отбор кандидатов 
$\rightarrow$ Ранжирование $\rightarrow$ Переранжирование с
учетом бизнес-требований $\rightarrow$ выдача

\subsection{Методы}

\begin{enumerate}
    \item Коллаборационная фильтрация:
    
    Используем только информацию о взаимодействиях

    $R = \begin{pmatrix}
        r_{u_1i_1} \ldots r_{un} \\
        r_{u_2i_1} \ldots \ldots
    \end{pmatrix}$

    \begin{enumerate}
        \item Memory-based методы
        
        Просто используем эвристики - 
        находим похожих пользователей и рекомендуем то,
        что понравилось им

        \item Модели со скрытыми переменными
        
        $p_u \in R^d$ - вектор пользователя

        $q_i \in R^d$ - вектор айтема

        Обучаем так, чтобы:

        $r_{ui} \approx <p_u, q_i>$

        $(\star) \sum_{u, i \in R} (r_{ui} - w_u - w_i - <p_u, q_i>) 
        \rightarrow \min_{\substack{p_u, q_i \\ w_u, w_i}}$

        Наблюдение 1:

        $R \in R^{n \times m}$

        $P = (p_1 \mid \ldots \mid p_n) \in R^{d \times n}$

        $Q = (q_1 \mid \ldots \mid q_n) \in R^{d \times m}$

        $(P^TQ)_{ui} = <p_u, q_i>$

        $\norm{R - P^TQ}_F \rightarrow \min_{P, Q}$

        Если матрица R известна, то это задача низкорангового 
        приближения R - SVD

        PureSVD: Заполняем все пропуски нулями и 
        применяем обычный SVD

        Наблюдение 2: 

        Если знаем все $q_i$

        Рассмотрим конкретного пользователя:

        $\sum_{i : \exists r_{ui}} (r_{ui} - w_u - 
        w_i - <p_u, q_i>)^2 \rightarrow \min_{p_u \in R^d}$

        Задача сводится к линейной регрессииU

        Где это может пригодится?
        \begin{enumerate}
            \item Обучили (*), получили P, Q
            - матрицу Q можно хранить на серверах

            Когда приходит u решаем задачу линейной регрессии,
            обучая $p_u$

            Не нужно хранить p

            Можно учесть самые свежие действия пользователя
            \item Можно фиксировать все $q_i$ на основе контента
            айтема i
        \end{enumerate}

        Обучение LFM:

        \begin{enumerate}
            \item SGD
            
            Задача невыпуклая - можно попасть в локальный минимум

            \item ALS (alternating least squares)
            
            Фиксируем P, находим Q 
            $\rightarrow$ Фиксируем Q, находим P
            $\rightarrow \ldots$
        \end{enumerate}

        Наблюдение 3:

        Если нашли идеальное решение $R = P^TQ$

        $\norm{r_i}$ - насколько всем пользователям
        нравится этот айтем

        Даже если много единиц, то норма все равно будет
        большой $\rightarrow$ популярность айтема i

        $r_i = P^Tq_i$

        $\sigma_{min}(P) \times \norm{q_i} \geq \norm{r_i} 
        \leq \sigma_{max}(P) \times \norm{q_i}$

        \rule{\linewidth}{0.5pt}

        \begin{center}
            \textit{Сингулярное разложение}
        \end{center}

        Пусть дана матрица $F_{n \times m}$. Тогда F
        можно представить в следующем виде:

        \[F_{n \times m} =
        U_{n \times n}
        \Sigma_{n \times m}
        V^T_{m \times m}\]

        Основные свойства сингулярного разложения:

        \begin{enumerate}
            \item $n \times n$-матрица $U=(u_1, \dots, u_n)$ 
            ортогональна, 
            $U^TU=I_n$,
            столбцы $u_j$ — собственные векторы матрицы $FF^T$
            \item $m \times m$-матрица $V=(v_1, \dots, v_n)$ 
            ортогональна, 
            $V^TV=I_m$,
            столбцы $v_j$ — собственные векторы матрицы $F^TF$
            \item $n \times m$-матрица $\Sigma_{n \times m}$
            диагональная, 
            $\Sigma_{n \times m} = diag(\sqrt{\lambda_1}, 
            \dots, \sqrt{\lambda_n})$,
            $\lambda_j \geq 0$ — собственные значения матриц 
            $F^TF$ и $FF^T$
            $\sqrt{\lambda_i}$ - сингулярные числа
        \end{enumerate}

        \rule{\linewidth}{0.5pt}

        Пусть i популярен - $\norm{q_i} \gg 0 \rightarrow$
        для многих пользователей $<p_u, q_i> \gg 0$ 

        Хак: Заменяем a(x) = $<p_u, q_i>$ на
        $a(x) = \frac{<p_u, q_i>}{\norm{p_u}\norm{q_i}}$

        Наблюдение 3':
        i: 5 показов, 4 успешных $\rightarrow$ высокий CTR
        $\rightarrow$ норма становится высокой, но 
        большой CTR может быть случайностью на низком количестве
        показов

        Надо регуляризировать с учетом норм $q_i, p_u$

        Наблюдение 4:

        LFM: $(\mid U \mid + \mid I \mid)(d + 1)$ параметров

        Увеличиваем количество параметров с помощью Neural CF:

        \begin{enumerate}
            \item Берем эмбеддинги $p_u, q_i$
            \item Конкатенируем эмбединги в один вектор
            \item Наворачиваем сверху полносвязные слои
            \item В конце они должны предсказывать $r_{ui}$
            
            Для задачи лучше использовать не полносвязные слои,
            чтобы не застревать в локальных минимумах
        \end{enumerate}

        Factorization Machines - обобщение LFM
    \end{enumerate}
\end{enumerate}

\subsection{Работа с неявным фидбэком}

Явный фидбэк: пользователь непосредственно поставил оценку

Неявный фидбэк: факт покупки, факт просмотра, ...

\textbf{iALS}

$S_{ui} = \begin{cases}
    1, \exists r_{ui} \\
    0, \not{\exists} r_{ui}
\end{cases}$

$C_{ui} = 1 + [\exists r_{ui}] \alpha r_{ui}$

$\alpha$ может принимать различные значения в зависимости 
от градации позитивности неявного фидбэка

Модель:

\[\sum_{\substack{u \in U \\ i \in I}} C_{ui}(S_{ui} - 
w_u - w_i - <p_u, q_i>)^2 + \lambda \dots\]

\subsection{Контентные рекомендации}

Подходы

\begin{enumerate}
    \item
    $q_i = f(i)$ - считаем $q_i$ по контенту

    Обучаем $(a_i)$
     
    \item DSSM, Siameese Nets
    
    $i \rightarrow$ контент $\rightarrow$ превращаем в вектор
    $q_i$

    $u \rightarrow$ история $(i_1, i_2, \dots) \rightarrow$
    превращаем в вектор $p_u$

    Требуем, чтобы $corr(\rho(p_u, q_i), r_{ui}) \uparrow$

    Для обучения используем триплетную функцию потерь
\end{enumerate}

\subsection{Как это все используется}

\begin{enumerate}
    \item Отбор кандидатов
    \begin{enumerate}
        \item Эвристики
        \item Легкая модель
        \item Поиск ближайших соседей
        \item Графовые методы
        \begin{enumerate}
            \item Двудольный граф с пользователями и айтемами
            \item Запускаем случайное блуждание
        \end{enumerate}
    \end{enumerate}
    \item Ранжирование
    $(u, i) \rightarrow$ признаки
\end{enumerate}

\section{AutoML}

\subsection{Что такое AutoML}

\begin{enumerate}
    \item Автоматизация некоторого этапа ML
    \item Система, которая способна полностью решать бизнес задача
\end{enumerate}

\textbf{Уровни AutoML}

\begin{enumerate}
    \item Сами алгоритмы
    \item API к алгоритмам
    \item Автоматическая оптимизация гиперпараметров / подбор ансамблей
    \item Автоматическая генерация признаков, 
    аугментаций, отбор признаков, визуализация
    \begin{enumerate}
        \item Стратегии обучения + управление бюджетом
        \item Простое Meta обучение
    \end{enumerate}
    \item Автоматическое определение домена, 
    объединение табличек без знания структуры базы данных, 
    спецификация под задачу
    \item Полная оптимизация, работает лучше, чем люди
\end{enumerate}

\textit{Перспективные направления}

\begin{enumerate}
    \item Продвинутое Meta learning
    \item Domain Specific Language
    \item Базы знаний
\end{enumerate}

Бывают проприетарные и открытые AutoML, так же
есть исследовательские и индстриальные

\subsection{Зачем нужен AutoML?}

\begin{enumerate}
    \item ML выгоден, AutoML быстрый - не нужно таких затрат на работу
    \item Автоматическое решение обходят только специалисты и для этого нужно время
\end{enumerate}

\subsection{Элементы AutoML}

\begin{enumerate}
    \item Данные
    \begin{enumerate}
        \item Нет предобработки
        \item Разные источники и форматы
        \item Структурированные и неструктурированные
    \end{enumerate}
    \item Black Box
    \begin{enumerate}
        \item Препроцессинг
        \item Генерация признаков
        \item Выбор гиперпараметров
        \item Обучение модели / ансамбля - оптимизация целевой метрики
    \end{enumerate}
    \item Предсказания
\end{enumerate}

Экспертная система:

\begin{enumerate}
    \item K-раз прогоняем препроцессинг с учетом уже построенных ранее моделей
    \item Генерация признаков + выбор гиперпараметров
    \item Обучение модели
\end{enumerate}

Нелинейная связь между элементами - результат каждого этапа 
может влиять на предыдущий

\subsection{Существующие решения}

\begin{enumerate}
    \item AutoSklearn
    \begin{enumerate}
        \item Умеет работать в сжатые сроки
        \item Оптимизируется байсовским оптимизатором
        \item Модели на каждой итерации байесовского оптимизатора сохраняются
        \item Для каждого датасета нашли оптимальный пайплайн и построили метамодель - 
        использование 25 оптимальных кандидатов и постройка ансамбля для похожих датасетов
    \end{enumerate}
    \item AutoSklearn 2.0
    \begin{enumerate}
        \item Увеличили размер метадатасета - разносторонние пайплайны
    \end{enumerate}
    \item Oboe
    \begin{enumerate}
        \item На основе метамоделей позволяет эффективно строить модели
        \item Признаки основаны на качестве модели, а не статистиках датасета
        \item Восстанавливаем матрицу ошибок простыми моделям
    \end{enumerate}
    \item TensorOboe
    \begin{enumerate}
        \item Вместо матрицы ошибок тензор ошибок
    \end{enumerate}
    \item TPOT
    \begin{enumerate}
        \item Строят дерево и оптимизируют его генетическим алгоритмом
    \end{enumerate}
    \item AutoGluon
    \begin{enumerate}
        \item Используют многоуровневые сети и LightGBM
        \item Делают бэггинг и K-fold валидации
        \item Дамп моделей занимает 200 гб
    \end{enumerate}
\end{enumerate}

\subsection{Анализ и выводы}

Слабые пайплайны:

\begin{enumerate}
    \item Простые / неэффективные модели
    \item Наивный препроцессинг и генерация признаков 
\end{enumerate}

Мета-алгоритмы:

\begin{enumerate}
    \item Маленькие наборы датасетов
    \item Синтетические, игрушечные и странные датасеты
    \item Слишком широкая сетка гиперпараметров
    \item Вычислительно дорогая оптимизация параметров
\end{enumerate}

\subsection{Бэнчмарки}

\begin{enumerate}
    \item OpenML
    \item AutoCV, AutoNLP, AutoTS, AutoSignal, ..., AutoDL
\end{enumerate}
\pagebreak

\part{Семинары}
\section{Семинар: Задачи условной оптимизации}

\textit{Учебник: Boyd, Convex Optimization}

\[\begin{cases}
    f_0(x) \rightarrow min_{x \in R^d} \\
    f_i(x) \leq 0, i = 1, \ldots, m \\
    h_i(x) = 0, i = 1, \ldots, p
\end{cases}\]

\[G(x) = f_0(x) + \sum_{i = 1}^m I_{-}(f_i(x)) + \sum_{i = 1}^p I_0(h_i(x)) \rightarrow min\]

Штрафы за нарушение ограничений:

\[I_{-}(z) = \begin{cases}
    0, z \leq 0 \\
    + \infty, z > 0 
\end{cases}\]

\[I_{0} = \begin{cases}
    0, z = 0 \\
    + \infty, z \neq 0 
\end{cases}\]

\(G(x) \rightarrow \infty\) в точках где не выполняется условие

Проблема: Недифференцируема

Заменяем функции на их аппроксимации ($\hat{I}_{-} = ax$)

Лагранжиан:

\[L(x, \lambda, \nu) = f_0(x) + \sum_{i = 1}^m \lambda_i f_i(x) + \sum_{i = 1}^p \nu_i h_i(x)\]
\[\lambda_i \geq 0\]

x - прямые (primal) переменные

$\lambda, \nu$ - двойственные переменные

\textbf{Двойственная функция}

\[g(\lambda, \nu) = \inf_{x} L(x, \lambda, \nu)\]

\begin{itemize}
    \item Двойственная функция всегда вогнутая
    \item Дает нижнюю оценку на минимум функции в прямой задаче
    
    \(x^{\prime}\) - допустимая точка (все условия выполнены)

    \[L(x^{\prime}, \lambda, \nu) = f_0(x^{\prime}) + 
    \sum_{i = 1}^m \lambda_i f_i(x^{\prime}) + \sum_{i = 1}^p \nu_i h_i(x^{\prime})\]
    \[f_i(x) \leq 0, h_i(x) = 0 \rightarrow 
    L(x^{\prime}, \lambda, \nu) \leq f_0(x^{\prime})\]
    \[\inf_x L(x, \lambda, \nu) \leq \inf_{x^{\prime}} 
    L(x', \lambda, \nu) \leq \inf_{x^{\prime}} f_0(x')\]

    $\uparrow$ - это и есть решение исходной задачи

    \[g(\lambda, \nu) \leq f_0(x_{\star})\]

    \[g(\lambda, \nu) \rightarrow \max_{\lambda, \nu}, \lambda_i \geq 0\]

    \(\lambda^{\star}, \nu^{\star}\) - решение двойственной задачи

    \(g(\lambda^{\star}, \nu^{\star}) \leq f_0(x_{*})\) - слабая двойственность

    \(g(\lambda^{\star}, \nu^{\star}) = f_0(x_{*})\) - сильная двойственность

    \underline{Достаточное условие сильной двойственности (Условие Слейтера)}

    \begin{itemize}
        \item Задача выпуклая:
        
        \(f_0, f_1, \ldots, f_m\) - выпуклые

        \(h_1, \ldots, h_p\) - линейные
        \item \(\exists x^{\prime}\), что все ограничения выполнены строго
    \end{itemize}
\end{itemize}

Пусть имеет место сильная двойственность:

\[g(\lambda^{\star}, \nu^{\star}) = f_0(x_{*})\]

\[g(\lambda^{\star}, \nu^{\star}) = \inf_x(f_0(x) + 
\sum \lambda^{\star} f_i(x) + \sum \nu^{\star} h_i(x))
\leq f_0(x_{\star}) + 
\sum \lambda^{\star} f_i(x_{\star}) + \sum \nu^{\star} h_i(x_{\star}) 
\leq f_0(x_{\star})\]

Все неравенства являются равенствами:

\begin{itemize}
    \item Если решить безусловную задачу при подставлении $\lambda^{\star}, \nu^{\star}$,
    то получим решение прямой задачи
    \item \(\lambda_i^{\star}f_{i}(x^{\star}) = 0\) - условие дополняющей нежесткости
\end{itemize}

\textbf{Теорема Куна-Такера}

Необходимые условия для 

\[
\begin{cases}
    \nabla_x L(x_{\star}, \lambda^{\star}, \nu^{\star}) = 0 \\
    f_i(x) \leq 0 \\
    h_i(x) = 0 \\
    \lambda_i \geq 0 \\
    \lambda_i f_i(x_{\star}) = 0 \\
    \textrm{Сильная двойственность}
\end{cases}
\leftrightarrow x_{\star}, \lambda^{\star}, \nu^{\star} \textrm{решения}
\]

\section{Семинар 3: EM алгоритм}

На М-шаге:

\[\Theta = \argmax_{\Theta} E_{q} log p(X, Z \mid \Theta)\]

\[log p(X \mid \Theta_{i + 1}) > log p(X \mid \Theta_{i})\]


Задача: \textbf{Шумная разметка изображений 100 экспертами}

i - изображение, j - эксперт: $l_{ij} \in \{0, 1\}$

Истинный класс для картинки $Z_i \in \{0, 1\}$

Дополнительные параметры:

\[\beta_i \in (0, +\infty), \alpha_j \in \mathcal{R}\]

$\beta$ - сложность изображения, $\alpha$ - уровень эксперта

\[p(l_{ij} = Z_i \mid Z_i, \alpha_j, \beta_i) = \sigma(\alpha_j \beta_i) 
= \frac{1}{1 + e^{-\alpha_j \beta_i}}\]

\[p(Z_i, l_i \mid \alpha, \beta) = 
p(Z_i) \prod_{j} p(l_ij \mid Z_i, \alpha_j, \beta_i)\]

\(p(Z_i)\)?

\begin{enumerate}
    \item Задать как 1/2, т.к. имеем два класса
    \item Задать как баланс классов
    \item Найти как параметр \(p(1) = \pi\)
\end{enumerate}

\[p(Z, l \mid \alpha, \beta) = 
\prod_i Z_i \prod_{j} p(l_ij \mid Z_i, \alpha_j, \beta_i)\]

Необходимо свести вероятность $l_{ij} = Z_i$ к вероятности $l_{ij}$

\[p(l_{ij} = Z_i \mid \ldots) = \sigma(\alpha\beta)\]

\[p(l_{ij} \neq Z_i \mid \ldots) = 1 - \sigma(\alpha\beta)\]

Бернулли:

\[p(l \mid \ldots) = p(l = Z \mid \ldots)^{[l = Z]} 
\times p(l \neq Z \mid \ldots)^{[l \neq Z]} =
\sigma(\alpha\beta)^{[l = Z]}\sigma(-\alpha\beta)^{[l \neq Z]}\]

\[p(Z_i, l_{ij} \mid \ldots) = 
p(Z_i)\prod_{j}\sigma(\alpha\beta)^{[l = Z]}
\sigma(-\alpha\beta)^{[l \neq Z]}\]

\textbf{Е-шаг:}

\[q^{\star}(Z_i) = p(Z_i \mid l_{ij}, \alpha_j, \beta_i) 
\xrightarrow{\textrm{Теорема Байеса}} \frac{p(Z_i, l_{ij}\mid\alpha,\beta)}
{p(l_{ij}\mid\alpha,\beta)} = \frac{p(Z_i, l_{ij}\mid\alpha,\beta)}
{\sum_{t} p(t, l_{ij}\mid\alpha,\beta)}\]

\[q^{\star}(Z) = \frac{p(Z_i)
\prod_{j}\sigma(\alpha\beta)^{[l = Z]}
\sigma(-\alpha\beta)^{[l \neq Z]}}
{\sum_{t \in \{0, 1\}} p(t_i)
\prod_{j}\sigma(\alpha\beta)^{[l = t]}
\sigma(-\alpha\beta)^{[l \neq t]}} 
= \frac{\gamma^{Z_i}_i}{\gamma^{0}_i + \gamma^{1}_i} = 
\frac{e^{log\gamma^{Z_i}_i}}
{e^{log\gamma^{0}_i + \gamma^{1}_i}}\]

\textbf{M-шаг:}

\[E_{q^{\star}} log p(Z, l \mid \alpha, \beta) 
\rightarrow \max_{\alpha, \beta}\]

\[E_{q^{\star}} log \prod_i p(Z, l \mid \alpha, \beta) = 
\sum_i E_{q^{\star}_i} logp(Z, l \mid \alpha, \beta) = \]
\[=\sum_i E_{q^{\star}_i}[logp(Z_i) + 
\sum_j [l_{ij} = Z_i] log \sigma(\alpha\beta) + 
[l_{ij} \neq Z_i] log \sigma(-\alpha\beta)] 
\rightarrow \max_{\alpha, \beta}\]

\[\sum_i \sum_j \sum_{t \in \{0, 1\}} 
q_i^{\star}(t)[[l_{ij} = t] log \sigma(\alpha\beta) + 
[l_{ij} \neq t] log \sigma(-\alpha\beta)]\]

Оптимизируем:

\[\frac{\partial}{\partial x} log\sigma(x) = \sigma(-x)\]

\[\frac{\partial}{\partial \alpha}
log\sigma(\alpha\beta)= 
\beta\sigma(-\alpha\beta)\]

\[\frac{\partial}{\partial \alpha} \log\sigma(-\alpha\beta) = 
-\beta\sigma(\alpha\beta)\]

\[\frac{\partial}{\partial \alpha} E_{q^{\star}} 
log p(Z, l \mid \alpha, \beta) = 
\sum_i \sum_t q_i^{\star} \beta ([l = t]\sigma(-\alpha\beta)) - 
[l \neq t]\sigma(\alpha\beta))\]

По $\beta$ аналогично

\section{Семинар 4: Основы байсовских методов}

Существует распределение $p(x, y)$

Интересует распределение: $p(y \mid x)$

\textit{Формула Байеса}

\[p(y \mid x) = \frac{p(x \mid y)p(y)}{p(x)}\]

$p(x \mid y)$ - правдоподобие X, распределение объектов
для некоторого класса

$p(y)$ - априорное распределение, 
доли классов в обучающей выборке

$p(x)$ - нормировочная константа

\textit{Функционал среднего риска}

\[R(a) = \int_Y \int_X L(y, a(x))p(x, y)dxdy\]

\[E_{y, x}L(y, a(x))\]

\textit{Как использовать оптимальное распределение, 
когда оно найдено?}

\[L(y, a) = [y \neq a]\]

Функционал среднего риска:

\[R(a) = \int_Y \int_X [y \neq a(x)]p(x, y)dxdy 
= \sum_{y = 1}^{K}\int_X[y \neq a(x)]p(x, y)dxdy =\]
\[= \int_X \sum_{y \neq a(x)}p(x, y)dxdy 
= \int_X (1 - \sum_{y = a(x)}p(x, y))dxdy =\]
\[1 - \int_X p(x, a(x))dxdy \rightarrow \min \Rightarrow
a_{\star}(x) = \argmax_{y \in Y} P(y \mid x)\]

Для регрессии:

\[L(y, a) = (y - a)^2\]
\[a_{\star}(x) = E(y \mid x)\]

\textit{Как найти $p(y \mid x)$}

В классификации: 

\[a_{\star}(x) = 
\argmax_{y \in Y} p(y \mid x) = 
\argmax_{y \in Y} \frac{p(x \mid y)p(y)}{p(x)} = \]
\[= \argmax_{y \in Y}p(x \mid y)p(y)\]

$p(y)$ задается исходя из распределения $y$

$p(x \mid y, \theta)$ находим $\theta$ ММП
\newline

\textit{Пример:}

\rule{\linewidth}{0.5pt}
\[p(y \mid x, w) = \mathcal{N}(<w, x>, \sigma^{2})\]

Правдоподобие:

\[\prod_{i = 1}^{\ell}\frac{1}{\sqrt{2 \pi \sigma^{2}}}
exp(-\frac{(y_i - <w, x_i>)^{2}}{2\sigma^{2}}) 
\rightarrow \max_{w}\]

\[logL = -\ell log\sqrt{2 \pi \sigma^{2}} - 
\frac{1}{2 \sigma^{2}}\sum_{i = 1}^{l}(y_i - <w, x_i>)^2
\rightarrow \max_{w} \Rightarrow\]
\[\Rightarrow \sum_{i = 1}^{l}(y_i - <w, x_i>)^2 
\rightarrow \min_{w}\]
\rule{\linewidth}{0.5pt}
\newline

Классификация:

Нужно найти $p(x \mid y, \theta)$ для всех классов

\[p(x \mid y, \theta) = \mathcal{N}(\mu_{y}, \Sigma_{y})\]

Можем найти $\mu_y, \Sigma_y$ по ММП

Если параметры распределены нормально - 
Нормальный дискриминантный анализ

Если $\Sigma_y = \Sigma$, метод называется
линейный дискриминант Фишера

Разделяющая поверхность:

\[p(y = +1 \mid x, \theta) = p(y = -1 \mid x, \theta)\]

$\Sigma_{-1} \neq \Sigma_{+1} \Rightarrow$ квадратичная поверхность

$\Sigma_{-1} = \Sigma_{+1} \Rightarrow$ Линейная поверхность

\rule{\linewidth}{0.5pt}

\textbf{Больше распределений:}

\[p(w \mid y, x) = \frac{p(y \mid x, w)p(w)}{p(x, y)}\]

$p(w) \sim \mathcal{N}(0, \sigma^{2}I)$ - запрещаем модели большие веса

\[logP(w \mid y, x) = 
-\frac{1}{2\sigma_{2}}\sum_{i = 1}^{\ell}(y_i - <w, x_i>)^2 -
\frac{\ell}{2\alpha^{2}}\sum_{j = 1}^{\alpha}w_{j}^{2} 
\rightarrow \max_{w}\]

Фактически: MSЕ с регуляризацией $L^2$

\[\lambda = \frac{\ell \sigma^{2}}{\alpha^{2}}\]

Что если $w_j \sim \mathcal{N}(0, \alpha_j^2)$?

Отдельный коэффициент регуляризации для каждого
параметра - такое не особо выводится в классическом 
машинном обучении $\rightarrow$ RVM

\rule{\linewidth}{0.5pt}

\textbf{Наивный Байесовский алгоритм}

Исходя из предположения о независимости признаков:

\[p(x \mid y) = \prod_{j = 1}^{d}p(x_j \mid y)\]
\[a(x) = \argmax_{y \in Y}p(y \mid x) = 
\argmax_{y}(lnP(y) + \sum_{j = 1}^d lnP(x_j \mid y))\]

\section{Семинар 5: Спектральная кластеризация}

\textbf{Алгоритм:}

\begin{enumerate}
    \item $L = D - W, 
    D = diag(d_1, \ldots, d_{\ell}), 
    d_i = \sum_{j = 1}^{\ell}w_{ij}$
    \item $u_1, \ldots, u_m$ - собственные векторы, соотв.
    минимальным собственным значениям L
    \item $U = (u_1 \mid \ldots \mid u_m) \in R^{l \times m}$
    \item K-means над U
\end{enumerate}

Почему не делать кластеризацию t-SNE или Umap?

\begin{enumerate}
    \item Оптимизирует положение точек, 
    а не расположение кластеров
    \item В t-SNE ошибка может быть неограничено большой
    $\rightarrow$ зашумленные представления объектов
    \item Есть шанс, что PSA будет лучше, чем t-SNE
\end{enumerate}

Задача кластеризации:

\[W(A, B) = \sum_{i \in A, j \in B}w_{ij}\]
\[A, B \subset X\]
\[A \cap B = \emptyset\]
\[\bar{A} = X \setminus A\]
\[\textrm{Ratio Cut}(A_1, \ldots, A_k) = 
\frac{1}{2} \sum_{i = 1}^K 
\frac{W(A_i, \bar{A}_i)}{\mid A_i \mid}
\rightarrow \min_{A_1, \ldots, A_K}\]

K = 2:

\[\textrm{Ratio Cut}(A, \bar{A}) 
\rightarrow \min_{A \subset X}\]

Задача поиска минимального разреза

\[X = A \cup \bar{A}\]

\[f: f_i =
\begin{cases}
    \sqrt{\frac{\mid\bar{A}\mid}{\mid A \mid}}, x_i \in A \\
    -\sqrt{\frac{\mid A\mid}{\mid \bar{A} \mid}}, x_i \in \bar{A}
\end{cases}\]

Квадратичная форма:

\begin{gather*}
    f^TLf = \frac{1}{2}\sum_{i, j = 1}^{\ell} 
    w_{ij}(f_i - f_j)^2 = \\
    = \frac{1}{2}\left(\sum_{x_i \in A, x_j \in \bar{A}} w_{ij}
    \sqrt{\frac{\mid\bar{A}\mid}{\mid A \mid}} +
    \sqrt{\frac{\mid A\mid}{\mid \bar{A} \mid}}\right)^2 +
    \frac{1}{2}\left(\sum_{x_i \in \bar{A}, x_j \in A} w_{ij}
    \sqrt{-\frac{\mid A \mid}{\mid \bar{A} \mid}} + 
    \sqrt{-\frac{\mid \bar{A} \mid}{\mid A \mid}}\right)^2 \\
    = \frac{1}{2}\left(\sqrt{\frac{\mid\bar{A}\mid}{\mid A \mid}} +
    \sqrt{\frac{\mid A\mid}{\mid \bar{A} \mid}}\right)^2
    (W(A, \bar{A}) + W(\bar{A}, A)) \Rightarrow \\
    \Rightarrow \left(\sqrt{\frac{\mid\bar{A}\mid}{\mid A \mid}} +
    \sqrt{\frac{\mid A\mid}{\mid \bar{A} \mid}}\right)^2
    = \left(\frac{\mid \bar{A} \mid + \mid A \mid}{\mid A \mid}
    + \frac{\mid A \mid + \mid \bar{A} \mid}{\mid \bar{A} \mid}\right)
    = \ell (\frac{1}{\mid A \mid} + \frac{1}{\mid \bar{A} \mid}); \\
    (W(A, \bar{A}) + W(\bar{A}, A)) = 2W(A, \bar{A}) \Rightarrow \\
    f^TLf = \ell(\frac{1}{\mid A \mid} + \frac{1}{\mid \bar{A} \mid})
    W(A, \bar{A}) = 2\ell\textrm{Ratio Cut}(A, \bar{A}) 
    \propto \textrm{Ratio Cut}(A, \bar{A}) \\
    \sum_{i = 1}^{\ell}f_i = \mid A \mid 
    \sqrt{\frac{\mid\bar{A}\mid}{\mid A \mid}} - 
    \mid \bar{A} \mid 
    \sqrt{\frac{\mid A \mid}{\mid \bar{A} \mid}}
    = \sqrt{\mid A \mid \mid \bar{A} \mid} - 
    \sqrt{\mid A \mid \mid \bar{A} \mid} = 0 \\
    \sum_{i = 1}^{\ell}f_i^2 = 
    \mid A \mid
    \frac{\mid\bar{A}\mid}{\mid A \mid} +
    \mid \bar{A} \mid 
    \frac{\mid A \mid}{\mid \bar{A} \mid}
    = \ell
\end{gather*}

Переписываем оптимизационную задачу:

\begin{gather*}
    \begin{cases}
        f^TLf \rightarrow \min_{f_i \in \{\ldots\}} \\
        <f, \vec{1}> = 0 \\
        \norm{f}^2 = \ell
    \end{cases} \\
    \textrm{Релаксация: } f_i \in R \\
    \begin{cases}
        f^TLf \rightarrow \min_{f \in R^{\ell}} \\
        <f, \vec{1}> = 0 \\
        \norm{f}^2 = \ell
    \end{cases} \\
\end{gather*}
\begin{gather*}
    \textrm{Лагранжиан:} \\
    \mathcal{L} = f^TLf + \lambda_1<f, \vec{1}> + 
    \lambda_2(\norm{f}_2 - \sqrt{\ell}) \\
    \nabla_f \mathcal{L} = 2L_f + 
    \lambda_1 \vec{1} +
    \lambda_2 \frac{1}{\norm{f}}f = 0 \mid \times \vec{1}^T \\
    2 \vec{1}^TLf + \lambda_1 \ell + 
    \lambda_2 \frac{1}{\norm{f}}<\vec{1}, f> = 0 \Rightarrow\\
    L1 = 0 \Rightarrow f^TL1 = 0 \Rightarrow \lambda_1 \ell = 0 \\
    \lambda_1 = 0 \\
    2Lf + \lambda_2 \frac{1}{\norm{f}}f = 0 \\
    Lf = -\frac{\lambda_2}{2\norm{f}}f \Rightarrow \\
    \textrm{f - собственный вектор L, соотв. с.з.} \mu \\
    f^TLf = \mu f^Tf = \norm{f}_{2}\mu = \ell \mu \\
    \textrm{Новая задача:}
    \begin{cases}
        \mu \rightarrow \min_{\mu \textrm{ - с.з L}, 
        f \textrm{- c.в.}} \\
        <f, \vec{1}> = 0 \\
        \norm{f} = \sqrt{\ell}
    \end{cases}
\end{gather*}
Если G-связный, то с.в. соотв. 0 с.з. не подходит
из-за невыполнения первого ограничения, в неполном графе
может подходить

Решение - это с.в., соотв. второму собств. знач.

Находим f $\rightarrow$ запускам K-means

\section{Семинар 6: Отбор признаков}

\subsection{Deep Clustering}

\begin{enumerate}
    \item Прогоняем объекты через нейросеть
    \item По векторным описаниям строим псевдоразметку
    с помощью KMeans
    \item Обучаем на псевдоразметке
    \item Повторяем каждую эпоху
\end{enumerate}

Даже необученная сеть не супер плохо размечает

Проблемы:

\begin{enumerate}
    \item Несбалансированные выборки - использование 
    взвешенных лоссов
    \item Наличие пустых кластеров - берем случайный центр 
    другого кластера и добавляем шум
    \item Делаем PCA перед кластеризацией, L2 нормализацию
    \item Сбрасываем линейный слой на каждой эпохе
\end{enumerate}

Есть возможности для улучшений:

\begin{enumerate}
    \item Использовать не PCA, а MLP
    \item Инициализируем матрицу классификатора в виде центроидов
\end{enumerate}

\subsection{Positional Encoding}

Более простая задача:

\begin{enumerate}
    \item Подаем сети координату точки
    \item Она восстанавливает цвет пикселя с координатами
    \item Можно воссоздавать 3D сцены (NERF)
\end{enumerate}

\subsection{Спектральный анализ}

Берем картинку и парсим ее на частоты с помощью
преобразования Фурье

При высоких частотах - быстрые изменения цвета

Обычный персептрон не умеет передавать высокие частоты

\subsection{Positional encoding}

Подаем не только саму картинку, но и гармоники - сеть
сможет извлекать высокие частоты и обучаться на них

\subsection{Feature extraction}

Методы:

\begin{enumerate}
    \item Filter
    \begin{enumerate}
        \item Relevancy - удаляем близкие фичи
        \item Redundancy - используем Mutual Info classifier
        \item MRMR classifier
    \end{enumerate}
    \item Wrapper - переучиваем модель на подмножествах фичей
\end{enumerate}

\section{Работа с признаками}

\begin{enumerate}
    \item Придумывание признаков
    \item Feature selection
    \item Понижение размерности
\end{enumerate}

\subsection{Отбор признаков}

\begin{enumerate}
    \item Методы фильтрации
    \begin{enumerate}
        \item Корреляция $x_j$ c $y$ - 
        не учитывает нелинейность и попарную корреляцию
        \item Для корреляции: t-score
        
        \[R_j = \frac{\mid\mu_{-1} - \mu_{+1}\mid}
        {\sqrt{\frac{\sigma_{-1}^2}{n_{-1}} +
        \frac{\sigma_{+1}^2}{n_{+1}}}}\]

        \item Для многоклассовой f-score
    \end{enumerate}
    \item Методы обертки
    \begin{enumerate}
        \item Ищем подмножество признаков, при
        котором ошибка модели на валидации поменьше
        \item Жадное удаление / добавление
        \item Генетические алгоритмы
        
        $\beta \in \{0, 1\}^{\alpha}$ - вхождение признака в подмножество
        признаков

        Итерация:

        \begin{enumerate}
            \item Популяция: $B = \{\beta_1, \ldots\}$
            \item Скрещивание: $\beta_j = \beta' \times \beta''$
            
            $\beta_j =
            \begin{cases}
                \beta', p = \frac{1}{2} \\
                \beta'', p = \frac{1}{2}
            \end{cases}$

            \item Мутация: $\sim \beta' \rightarrow \beta_j =
            \begin{cases}
                \beta_j', p \\
                1 - \beta_j', 1 - p
            \end{cases}$ 

            \item Новая популяция: $B' = \{\sim \beta' 
            \times \beta''\}$ для какого-то числа пар из B

            \item Делаем селекцию: Оставляем n лучших организмов
        \end{enumerate}
        \item Отбор признаков на основе моделей: Лассо, Out of bag
    \end{enumerate}
\end{enumerate}

\rule{\linewidth}{0.5pt}

\subsection{Понижение размерности}

Метод главных компонент (PCA)

\begin{align*}
    & X \in R^{1\times D}\\
    & u_1, \ldots, u_D \in R^D \textrm{ - главные компоненты, если} \\
    &(1): <u_i, u_j> = 0, \forall i \neq j \\
    &(2): \norm{u_j}^2 = 1 \\
    &(3): \textrm{ При проецировании выборки X на } u_1, \ldots, u_d:
    var \rightarrow \max
\end{align*}

Поиск первой компоненты:

\[\begin{cases}
    u_1^T X^T X u_1 \rightarrow \max\\
    \norm{u_1}^2 = 1
\end{cases}\]

Лагранжиан:

\[2X^TX u_1 + 2\lambda u_1 = 0 \Rightarrow 
\lambda \rightarrow \max\]

$u_1$ - собств. вектор $X^TX$ соотв. наибольшему с.з.

Постановка 2:

\begin{align*}
    & X \in R^{\ell \times D} \\
    & Z \in R^{\ell \times d} \\
    & U \in R^{d \times D} \\
    & \textrm{Задача:} \\
    & \norm{X - ZU^T}_F^2 \rightarrow min
\end{align*}

Решается с помощью сингулярного разложения

\section{Метод k ближайших соседей}

\begin{align*}
    & X = \{(x_i, y_i)\}_{i = 1}^{\ell} \\
    & \rho: X \times X \rightarrow (0, +\infty) \\
    & U: \rho(u, x_1) \leq \ldots \leq \rho(u_{\ell}) \\
    & a(u) = \argmax_{y \in Y} \sum_{i = 1}^{K} w_i [y_i = y] 
\end{align*}

Особенности метода:

\begin{enumerate}
    \item Шумовые признаки
    
    Очень чувствителен к шумовым признакам, 
    т.к. использует все признаки для 
    подсчета расстояния

    \item Проклятие размерности
    
    Все объекты находятся по краям гиперкуба - трудно быстро искать 
    близких соседей

    \item Функции расстояния
    \begin{enumerate}
        \item Метрика Минковского: 
        $\rho_p(x, z) = (\sum \mid x_j - z_j \mid^p)^{\frac{1}{p}}$

        $\rho_{\infty}(x, z) = max \mid x_j - z_j \mid$

        $\rho_0(x, z) = \sum_{j = 1}^d [x_j \neq z_j]$
        
        Можно добавить веса для отдельных признаков:

        Веса можно подбирать покоординатным спуском
        \item Расстояние Махаланобиса
        
        $\rho(x, z) = \sqrt{(x - z)^T S^{-1} (x - z)}$

        $S$ - симметричная, положительно определенная матрица

        \item Косинусная мера
        

        $\rho_{cos}(x, z) = 
        arccos(\frac{<x, z>}{\norm{x}\norm{z}})$
    \end{enumerate}
\end{enumerate}

\section{Метрические методы 2}

\subsection{Расстояния на категориальных признаках}

\underline{Один категориальный признак}

Как измерить расстояния:

\begin{enumerate}
    \item $\rho(x, z) = [x \neq z]$
    \item $\rho(x, z) = \alpha[x \neq z] + \beta[x = z]$
    \item Сделать $\alpha, \beta$ зависимыми от признака
    \item $\rho(x, z) = [x \neq z]log(f(x) + 1)log(f(z) + 1)$
    
    f(x) - сколько раз в обучающей выборке 
    встречается категория x

    \item $\rho(x, z) = [x \neq z] + [x = z] \times 
    \sum_{q: p(q) \leq p(x) p_{j}^2(q)}$

    $p(x)$ - частота

    $p_{j}^2(x)$ - вероятность, что у пары объектов категория x
\end{enumerate}

\subsection{Обучение метрик}

Зачем:

\begin{enumerate}
    \item Подобрать метрику для улучшения kNN
    \item Когда необходимо разносить разные объекты по дальности
\end{enumerate}

Самая параметризованная метрика: Метрика Махаланобиса

$\rho(x, z) = \norm{Ax  - Az}^2 = (x - z)^T A^TA (x - z)$

Выучиваем матрицу $A \in R^{n \times d}$

Методы обучения:

\begin{enumerate}
    \item NCA - neighborhood component analysis
    
    $x_i \rightarrow$ выбираем $x_j$ из некоторого распределения
    $\rightarrow$ относим $x_i$ к $y_j$

    \[p_{ij} = 
    \begin{cases}
        \frac{exp(- \norm{Ax_i - Ax_j}^2)}
        {\sum_{i \neq j} exp(- \norm{Ax_i - Ax_j}^2)}, i \neq j \\
        0, i = j
    \end{cases}\]

    Вероятность отнесения к правильному классу:

    $C_i = \{j \mid y_j = y_i\}$

    $p_i = \sum_{j \in C_i}p_{ij}$

    $Q(A) = \sum_{i = 1}^{\ell}p_i \rightarrow \max_{A}$
    \item LMNN - Large Margin NN
    
    Используем триплетный лосс:

    Берем для $x_i$ положительные объекты и отрицательные объекты

    $\eta_{ij} \in \{0, 1\}$ - является ли $x_j$ 
    целевым объектом для $x_i$ (входит в k соседей)

    Целевые объекты должны быть близки:

    \begin{multline*}
        \sum_{i \neq j} \xi_{ij}\norm{Ax_i - Ax_j}^2
        + C\sum_{i = 1}^{\ell}\sum_{j \neq i}
        \sum_{\substack{m \neq i \\ m \neq j}}\xi_{ij}
        [y_m \neq y_i] \times \\
        \times \max(0, \alpha + 
        \norm{Ax_i - Ax_j}^2 - \norm{Ax_i - Ax_m}^2)
        \rightarrow \min_{A}
    \end{multline*}

    \item ITML
    
    $p(x \mid A) = \frac{1}{z}exp(-\frac{1}{2}\norm{Ax - A\mu}^2)$

    z - нормировочная константа

    $\mu$ - центр распределения

    S - мн-во пар, которые похожи

    D - мн-во пар, которые не похожи

    $A_0$ - априорная матрица для расстояния Махаланобиса:

    Можно ввеси на основе выборочной ков. матрицы

    Можно ввести как диагональную

    \[\begin{cases}
        KL(p(x \mid A_0) \mid\mid p(x \mid A)) \rightarrow \min_{A} \\
        \rho_A(x_i, x_j) \leq u, (i, j) \in S \\
        \rho_A(x_i, x_j) \geq L, (i, j) \in D
    \end{cases}\]

    \item MCML (Maximally collapsing metric learning)
    
    $p_A(j \mid i) = \frac{exp(- \norm{Ax_i - Ax_j}^2)}
    {\sum_{k \neq i}exp(- \norm{Ax_i - Ax_k}^2)}$

    $p_0(j \mid i) \propto
    \begin{cases}
        1, y_i = y_i \\
        0, y_j \neq y_i
    \end{cases}$

    \[\sum_{i = 1}^{\ell} KL(p_0(\dot \mid i) 
    \mid \mid p_A(\dot \mid i)) \rightarrow \min_A\]

    \item Ядровой переход
    
    $K, \phi: X \to H$

    $L: H \to R^n$

    $\rho(x, z) = \norm{L\phi(x) - L\phi(z)}^2$

    Ищем L:

    Из функционального анализа:

    \(L(h) = (<h, w_1>, \ldots, <h, w_n>), w_1, \ldots, w_n \in H\)

    \(w_i = \sum_{j = 1}^{\ell} \alpha_{ij}\phi(x_j)\)
\end{enumerate}
\end{document}
